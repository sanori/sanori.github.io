<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-54TR4FSRPW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-54TR4FSRPW');
</script>
<!-- End Google Analytics -->

  
  <title>Writing Spark batches only in SQL | Sanori&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Apache SparkTM is known as popular big data framework which is faster than Hadoop MapReduce, easy-to-use, and fault-tolerant. Most of the Spark tutorials require readers to understand Scala, Java, or">
<meta property="og:type" content="article">
<meta property="og:title" content="Writing Spark batches only in SQL">
<meta property="og:url" content="http://sanori.github.io/2017/10/Writing-Spark-batches-only-in-SQL/index.html">
<meta property="og:site_name" content="Sanori&#39;s Blog">
<meta property="og:description" content="Apache SparkTM is known as popular big data framework which is faster than Hadoop MapReduce, easy-to-use, and fault-tolerant. Most of the Spark tutorials require readers to understand Scala, Java, or">
<meta property="og:locale">
<meta property="og:image" content="http://sanori.github.io/2017/10/Writing-Spark-batches-only-in-SQL/spark-doc-sql.png">
<meta property="article:published_time" content="2017-10-07T14:57:03.000Z">
<meta property="article:modified_time" content="2018-11-26T17:19:27.924Z">
<meta property="article:author" content="Joo-Won Jung">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="SQL">
<meta property="article:tag" content="SparkSQL">
<meta property="article:tag" content="DataFrame">
<meta property="article:tag" content="Dataset">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://sanori.github.io/2017/10/Writing-Spark-batches-only-in-SQL/spark-doc-sql.png">
  
    <link rel="alternate" href="/atom.xml" title="Sanori's Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Sanori&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">The world that sanori perceives</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/links">Links</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://sanori.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Writing-Spark-batches-only-in-SQL" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/10/Writing-Spark-batches-only-in-SQL/" class="article-date">
  <time class="dt-published" datetime="2017-10-07T14:57:03.000Z" itemprop="datePublished">2017-10-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Writing Spark batches only in SQL
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="http://spark.apache.org/">Apache Spark<sup>TM</sup></a> is known as popular big data framework which is faster than Hadoop MapReduce, easy-to-use, and fault-tolerant. Most of the Spark tutorials require readers to understand Scala, Java, or Python as base programming language. But, in my opinion, SQL is enough to write a spark batch script.</p>
<p>In this article, I will show that <strong>you can write Spark batches only in SQL if your input data is ready as structured dataset</strong>. This means that you don’t need to learn Scala or Python, RDD, DataFrame if your job can be expressed in SQL. Moreover, the expression power of SparkSQL may be stronger than you think.</p>
<p>The SQL scripts as follows can be found at <a target="_blank" rel="noopener" href="https://github.com/sanori/spark-sql-example">https://github.com/sanori/spark-sql-example</a>. You can test them yourself.</p>
<span id="more"></span>

<h2 id="Spark-batches"><a href="#Spark-batches" class="headerlink" title="Spark batches"></a>Spark batches</h2><p>Typical Spark batches are a program that</p>
<ol>
<li>read data from data sources,</li>
<li>transform and calculate the data, and</li>
<li>save the result.</li>
</ol>
<p>Most of the Spark tutorials require Scala or Python (or R) programming language to write a Spark batch. For example, you may write a Python script to calculate the lines of each plays of Shakespeare when you are provided the full text in parquet format as follows. (Some codes are included for illustration purpose.)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> sf</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start Spark</span></span><br><span class="line">spark = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">&quot;pyspark example&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">basedir = os.path.dirname(os.path.realpath(__file__))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read all of Shakespeare&#x27;s plays</span></span><br><span class="line">df = spark.read.parquet(os.path.join(basedir, <span class="string">&quot;data/shakespeare.gz.parquet&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the schema to the console</span></span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate number of lines of each work (play)</span></span><br><span class="line">result = df \</span><br><span class="line">    .groupBy(<span class="string">&quot;play_name&quot;</span>) \</span><br><span class="line">    .agg(sf.count(<span class="string">&quot;line_id&quot;</span>).alias(<span class="string">&quot;lines&quot;</span>)) \</span><br><span class="line">    .orderBy(<span class="string">&quot;lines&quot;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print a part of the result to the console</span></span><br><span class="line">result.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the result as one file in JSON Lines format</span></span><br><span class="line">result \</span><br><span class="line">    .repartition(<span class="number">1</span>) \</span><br><span class="line">    .write \</span><br><span class="line">    .json(os.path.join(basedir, <span class="string">&quot;length_of_play&quot;</span>), mode=<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Stop Spark</span></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p>The data calculation part of the above code is to get the <code>result</code> DataFrame, which is from line 20 to 23. This part can be written using SQL as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calculate number of lines of each work (play)</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;shakespeare&quot;</span>)  <span class="comment"># register df as shakespeare table</span></span><br><span class="line">result = spark.sql(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">SELECT play_name, count(line_id) AS lines</span></span><br><span class="line"><span class="string">FROM shakespeare</span></span><br><span class="line"><span class="string">GROUP BY play_name</span></span><br><span class="line"><span class="string">ORDER BY lines DESC</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>As I learn more about Spark, I realized that most of the data processing code can be replaced by SQL. And I found that most of the Spark batch script can be rewritten in SQL. For example, the above batch script can be rewritten in SQL as follows:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Read all of Shakespeare&#x27;s plays</span></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">VIEW</span> shakespeare</span><br><span class="line">  <span class="keyword">USING</span> parquet</span><br><span class="line">  OPTIONS (path &quot;data/shakespeare.gz.parquet&quot;);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Print the table schema and additional informations to the console</span></span><br><span class="line"><span class="keyword">DESCRIBE</span> EXTENDED shakespeare;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Calculate number of lines of each work and print to the console</span></span><br><span class="line"><span class="keyword">SELECT</span> play_name, <span class="built_in">count</span>(line_id) <span class="keyword">AS</span> lines</span><br><span class="line">  <span class="keyword">FROM</span> shakespeare</span><br><span class="line">  <span class="keyword">GROUP</span> <span class="keyword">BY</span> play_name</span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> lines <span class="keyword">DESC</span></span><br><span class="line">  LIMIT <span class="number">20</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Save the result as one file in JSON Lines format</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> IF <span class="keyword">EXISTS</span> lengthOfPlay;   <span class="comment">-- to overwrite, remove existing table</span></span><br><span class="line"><span class="keyword">SET</span> spark.sql.shuffle.partitions<span class="operator">=</span><span class="number">1</span>;   <span class="comment">-- to make single output file</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> lengthOfPlay</span><br><span class="line">  <span class="keyword">USING</span> json</span><br><span class="line">  LOCATION &quot;length_of_play&quot;</span><br><span class="line">  <span class="keyword">AS</span> <span class="keyword">SELECT</span> play_name, <span class="built_in">count</span>(line_id) <span class="keyword">AS</span> lines</span><br><span class="line">    <span class="keyword">FROM</span> shakespeare</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> play_name</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> lines <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>

<p>The above SQL script can be executed by <code>spark-sql</code> which is included in default Spark distribution. This may imply that Spark creators consider SQL as one of the main programming language. In fact, most of the SQL references are from the official Spark programming guide named <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL, DataFrames and Datasets Guide</a>. To see the SQL examples, you should click <strong>“Sql”</strong> tab as follows:</p>
<img src="/2017/10/Writing-Spark-batches-only-in-SQL/spark-doc-sql.png" class="" title="SQL tab on Spark official document">

<h2 id="Writing-SQL-only-script"><a href="#Writing-SQL-only-script" class="headerlink" title="Writing SQL only script"></a>Writing SQL only script</h2><p>The most problematic part of writing a Spark script only in SQL would be file operations. Most of the Spark tutorials suggest to use Scala or Python Language to read a data file and&#x2F;or save the result of data processing. But, there are several ways to read or write a data file in SparkSQL as follows.</p>
<p>The file path in the following examples can be a HDFS URI path. That is, you can write SQL script for cluster or distributed environment.</p>
<h3 id="Read-a-data-file"><a href="#Read-a-data-file" class="headerlink" title="Read a data file"></a>Read a data file</h3><h4 id="Read-directly-from-SQL-FROM"><a href="#Read-directly-from-SQL-FROM" class="headerlink" title="Read directly from SQL FROM"></a>Read directly from SQL <code>FROM</code></h4><p>You can simply read the data as a table using SQL <code>FROM</code> statement.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> play_name</span><br><span class="line"><span class="keyword">FROM</span> parquet.`data<span class="operator">/</span>shakespeare.gz.parquet`</span><br></pre></td></tr></table></figure>
<p>Reference: <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-programming-guide.html#run-sql-on-files-directly">http://spark.apache.org/docs/latest/sql-programming-guide.html#run-sql-on-files-directly</a></p>
<h4 id="Register-a-file-as-VIEW-or-TABLE"><a href="#Register-a-file-as-VIEW-or-TABLE" class="headerlink" title="Register a file as VIEW or TABLE"></a>Register a file as <code>VIEW</code> or <code>TABLE</code></h4><p>You may use data definition language such as <code>CREATE VIEW</code> or <code>CREATE TABLE</code> if you want to load data as a table or a view. I recommend to use <code>CREATE TEMPORARY VIEW</code> if you don’t want to modify the original data since a <code>TABLE</code> can be modified by <code>INSERT INTO</code>, <code>TRUNCATE</code>, etc.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">VIEW</span> shakespeare</span><br><span class="line"><span class="keyword">USING</span> parquet</span><br><span class="line">OPTIONS (path &quot;data/shakespeare.gz.parquet&quot;)</span><br></pre></td></tr></table></figure>
<p>Reference:</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-programming-guide.html#loading-data-programmatically">http://spark.apache.org/docs/latest/sql-programming-guide.html#loading-data-programmatically</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/create-view.html">https://docs.databricks.com/spark/latest/spark-sql/language-manual/create-view.html</a></li>
</ul>
<h3 id="Write-a-result-to-a-file"><a href="#Write-a-result-to-a-file" class="headerlink" title="Write a result to a file"></a>Write a result to a file</h3><p>In Spark tutorials with Scala or Python, the processed result can be saved to a file using <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@write:org.apache.spark.sql.DataFrameWriter[T]"><code>write</code></a> API of Dataset. This is intuitive for most programmers.</p>
<p>In SQL, all the data are abstracted as tables. Therefore, you should create a table to save or write the resultant data, instead of searching for write API for SQL.</p>
<h4 id="CREATE-TABLE-AS-SELECT"><a href="#CREATE-TABLE-AS-SELECT" class="headerlink" title="CREATE TABLE AS SELECT"></a><code>CREATE TABLE AS SELECT</code></h4><p>If you save a new dataset, you may use <code>CREATE TABLE AS SELECT</code> statement.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> lengthOfPlay</span><br><span class="line">  <span class="keyword">USING</span> json</span><br><span class="line">  LOCATION &quot;length_of_play&quot;</span><br><span class="line">  <span class="keyword">AS</span> <span class="keyword">SELECT</span> play_name, <span class="built_in">count</span>(line_id) <span class="keyword">AS</span> lines</span><br><span class="line">    <span class="keyword">FROM</span> shakespeare</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> play_name</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> lines <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>

<h4 id="INSERT-SELECT"><a href="#INSERT-SELECT" class="headerlink" title="INSERT SELECT"></a><code>INSERT SELECT</code></h4><p>If you want to append new data to existing table or directory,
you may use <code>INSERT INTO &lt;table&gt; SELECT</code> statement.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> lengthOfPlay</span><br><span class="line">  <span class="keyword">SELECT</span> play_name, <span class="built_in">count</span>(line_id) <span class="keyword">AS</span> lines</span><br><span class="line">  <span class="keyword">FROM</span> shakespeare</span><br><span class="line">  <span class="keyword">GROUP</span> <span class="keyword">BY</span> play_name</span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> lines <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>

<p>The table where the data are added must be created before to use <code>INSERT</code>.
A typical table definition example is as follows.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> lengthOfPlay (</span><br><span class="line">  play_name STRING,</span><br><span class="line">  lines <span class="type">INT</span></span><br><span class="line">  )</span><br><span class="line">  <span class="keyword">USING</span> json</span><br><span class="line">  LOCATION &quot;length_of_play&quot;;</span><br></pre></td></tr></table></figure>

<p>For more information, consult <a target="_blank" rel="noopener" href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/insert.html">https://docs.databricks.com/spark/latest/spark-sql/language-manual/insert.html</a></p>
<p>You may consider <code>INSERT OVERWRITE TABLE</code> statement to create a new dataset.
But, in my humble opinion, <code>CREATE TABLE AS SELECT</code> is better to create new dataset
since the table should be created before <code>INSERT</code> statement,
and there is a chance of schema inconsistency between <code>CREATE TABLE</code> and <code>INSERT SELECT</code>.</p>
<p>Note that there is no <code>UPDATE</code> statement in SparkSQL
since all the data in Spark are immutable, or not changeable.
You may create, append, or delete data, but you cannot change existing data.</p>
<h2 id="Running-SQL-only-script"><a href="#Running-SQL-only-script" class="headerlink" title="Running SQL only script"></a>Running SQL only script</h2><h3 id="spark-sql"><a href="#spark-sql" class="headerlink" title="spark-sql"></a><code>spark-sql</code></h3><p>If you have installed Spark, you can execute the SQL script file as follows:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$SPARK_HOME</span>/bin/spark-sql -f &lt;script.sql&gt;</span><br></pre></td></tr></table></figure>
<p>If you have added the spark executables to your <code>PATH</code>, <code>spark-sql -f &lt;script.sql&gt;</code> is enough.</p>
<p>Like <code>spark-submit</code>, <code>spark-sql</code> supports <code>--master</code>, <code>--conf</code>, <code>--executor-memory MEM</code>, <code>--executor-cores NUM</code> options. Therefore you can tune Spark parameters for better performance.</p>
<p>For more information on options and its meaning, consult <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit">http://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit</a></p>
<h3 id="Setting-parameters-by-SET"><a href="#Setting-parameters-by-SET" class="headerlink" title="Setting parameters by SET"></a>Setting parameters by <code>SET</code></h3><p>You can set Spark parameters using <code>SET</code> statement in SparkSQL instead of setting parameters in <code>spark-sql</code> options. For example, you can replace <code>--master</code> option in <code>spark-sql</code> as follows.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> spark.master<span class="operator">=</span>&quot;local[*]&quot;;</span><br></pre></td></tr></table></figure>

<p>For more configuration variables, consult <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/configuration.html#available-properties">http://spark.apache.org/docs/latest/configuration.html#available-properties</a></p>
<p>Reference: <a target="_blank" rel="noopener" href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/set.html">https://docs.databricks.com/spark/latest/spark-sql/language-manual/set.html</a></p>
<h2 id="Difference-from-RDBMS"><a href="#Difference-from-RDBMS" class="headerlink" title="Difference from RDBMS"></a>Difference from RDBMS</h2><p>Although Spark supports SQL including data definition language, Spark is <strong>NOT</strong> a relational DBMS. Spark just have taken SQL as data processing language.</p>
<p>All the data in Spark are immutable. That is, you <strong>cannot update</strong> existing tables, rows, and columns. There is no <code>UPDATE</code> statement.</p>
<h2 id="Motivation-of-this-article"><a href="#Motivation-of-this-article" class="headerlink" title="Motivation of this article"></a>Motivation of this article</h2><ul>
<li>Increasing Boilerplates in Spark batch scripts<ul>
<li>Boilerplate codes are increasing as time goes by and it makes hard to manage.</li>
<li>Most of Python&#x2F;Scala batches are the same except SQL part which process data.</li>
<li>Shell scripts are also required to submit Spark jobs using <code>spark-submit</code> with parameters.</li>
</ul>
</li>
<li>SparkSQL is similar to Dataset API<ul>
<li>Writing Dataset processing code using Dataset API is very similar to write SQL script.</li>
<li>Most of the Spark batches can be expressed in SQL language only.</li>
<li>SparkSQL examples exist in Dataset API.</li>
</ul>
</li>
<li>Dataset is recommended than RDD since Spark 2<ul>
<li>Dataset, which is general type of DataFrame, is supported by query optimizer (Catalyst) and efficient code generator and serializer (Tungsten).</li>
<li>It is recommended to use Dataset instead of RDD due to the efficiency when the developer is not a Spark expert.</li>
</ul>
</li>
</ul>
<h2 id="Further-Works"><a href="#Further-Works" class="headerlink" title="Further Works"></a>Further Works</h2><p>The expresion power of SparkSQL may be more than we think.</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window$">Window functions</a></li>
<li><a target="_blank" rel="noopener" href="https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html">Higher order functions</a></li>
</ul>
<p>I will write a article about these topics when I get time.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li>Spark SQL, DataFrames and Datasets Guide: <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a></li>
<li>Spark SQL Reference: <a target="_blank" rel="noopener" href="https://docs.databricks.com/spark/latest/spark-sql/index.html">https://docs.databricks.com/spark/latest/spark-sql/index.html</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://sanori.github.io/2017/10/Writing-Spark-batches-only-in-SQL/" data-id="clrgl30ub002ayzjd27fp4t2v" data-title="Writing Spark batches only in SQL" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
        <a href="http://sanori.github.io/2017/10/Writing-Spark-batches-only-in-SQL/#disqus_thread" class="article-comment-link"><span class="fa fa-comment">Comments</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DataFrame/" rel="tag">DataFrame</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dataset/" rel="tag">Dataset</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SQL/" rel="tag">SQL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SparkSQL/" rel="tag">SparkSQL</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/03/Running-Keras-directly-on-Tensorflow/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Running Keras directly on TensorFlow
        
      </div>
    </a>
  
  
    <a href="/2017/07/Running-Spark-on-Scala-Worksheet/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Running Spark on Scala Worksheet</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/2DArray/" style="font-size: 10px;">2DArray</a> <a href="/tags/BigInt/" style="font-size: 10px;">BigInt</a> <a href="/tags/CLI/" style="font-size: 10px;">CLI</a> <a href="/tags/CUDA/" style="font-size: 10px;">CUDA</a> <a href="/tags/CommandLineInterface/" style="font-size: 13.33px;">CommandLineInterface</a> <a href="/tags/DataFrame/" style="font-size: 10px;">DataFrame</a> <a href="/tags/Dataset/" style="font-size: 10px;">Dataset</a> <a href="/tags/JavaScript/" style="font-size: 20px;">JavaScript</a> <a href="/tags/Keras/" style="font-size: 10px;">Keras</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/REPL/" style="font-size: 10px;">REPL</a> <a href="/tags/React/" style="font-size: 10px;">React</a> <a href="/tags/ReactHooks/" style="font-size: 10px;">ReactHooks</a> <a href="/tags/SQL/" style="font-size: 16.67px;">SQL</a> <a href="/tags/Scala/" style="font-size: 13.33px;">Scala</a> <a href="/tags/Scala-Worksheet/" style="font-size: 10px;">Scala Worksheet</a> <a href="/tags/Spark/" style="font-size: 16.67px;">Spark</a> <a href="/tags/SparkSQL/" style="font-size: 16.67px;">SparkSQL</a> <a href="/tags/SqlWindowFunction/" style="font-size: 13.33px;">SqlWindowFunction</a> <a href="/tags/TensorFlow/" style="font-size: 13.33px;">TensorFlow</a> <a href="/tags/bitwise/" style="font-size: 10px;">bitwise</a> <a href="/tags/docker/" style="font-size: 13.33px;">docker</a> <a href="/tags/interactive-development/" style="font-size: 10px;">interactive development</a> <a href="/tags/nVidia/" style="font-size: 10px;">nVidia</a> <a href="/tags/node-js/" style="font-size: 20px;">node.js</a> <a href="/tags/nvidia-docker/" style="font-size: 13.33px;">nvidia-docker</a> <a href="/tags/sbt/" style="font-size: 10px;">sbt</a> <a href="/tags/spark-shell/" style="font-size: 10px;">spark-shell</a> <a href="/tags/toFixed/" style="font-size: 10px;">toFixed</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/01/Handling-Large-Integers-in-JavaScript/">Handling Large Integers in JavaScript</a>
          </li>
        
          <li>
            <a href="/2021/04/Migrate-to-useEffect/">Migrate to useEffect</a>
          </li>
        
          <li>
            <a href="/2019/08/Compare-Two-Tables-in-SQL/">Compare Two Tables in SQL</a>
          </li>
        
          <li>
            <a href="/2019/05/JavaScript-Pitfalls-Tips-2D-Array-Matrix/">JavaScript Pitfalls &amp; Tips: 2D Array, Matrix</a>
          </li>
        
          <li>
            <a href="/2019/04/JavaScript-Pitfalls-Tips-toFixed/">JavaScript Pitfalls &amp; Tips: toFixed</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
        <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></br>
This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.</br>
      
      &copy; 2024 Joo-Won Jung<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/links" class="mobile-nav-link">Links</a>
  
</nav>
    
<script>
  var disqus_shortname = 'sanoriblog';
  
  var disqus_url = 'http://sanori.github.io/2017/10/Writing-Spark-batches-only-in-SQL/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>



<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>