<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sanori&#39;s Blog</title>
  
  <subtitle>The world that sanori perceives</subtitle>
  <link href="http://sanori.github.io/atom.xml" rel="self"/>
  
  <link href="http://sanori.github.io/"/>
  <updated>2022-06-14T15:21:55.890Z</updated>
  <id>http://sanori.github.io/</id>
  
  <author>
    <name>Joo-Won Jung</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Migrate to useEffect</title>
    <link href="http://sanori.github.io/2021/04/Migrate-to-useEffect/"/>
    <id>http://sanori.github.io/2021/04/Migrate-to-useEffect/</id>
    <published>2021-04-07T18:07:50.000Z</published>
    <updated>2022-06-14T15:21:55.890Z</updated>
    
    <content type="html"><![CDATA[<p>TL;DR</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> &#123; useRef, useEffect &#125; <span class="keyword">from</span> <span class="string">&#x27;react&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> mounted = <span class="title function_">useRef</span>(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"><span class="title function_">useEffect</span>(<span class="function">() =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (mounted.<span class="property">current</span>) &#123;</span><br><span class="line">    mounted.<span class="property">current</span> = <span class="literal">true</span>;</span><br><span class="line">    <span class="comment">// componentDidMount part</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// componentDidUpdate part</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="function">() =&gt;</span> &#123;</span><br><span class="line">    <span class="comment">// componentWillUnmount part</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;, <span class="title class_">Array</span>_of_subscribed_props_and_states)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;TL;DR&lt;/p&gt;
&lt;figure class=&quot;highlight js&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;</summary>
      
    
    
    
    
    <category term="React" scheme="http://sanori.github.io/tags/React/"/>
    
    <category term="ReactHooks" scheme="http://sanori.github.io/tags/ReactHooks/"/>
    
  </entry>
  
  <entry>
    <title>Compare Two Tables in SQL</title>
    <link href="http://sanori.github.io/2019/08/Compare-Two-Tables-in-SQL/"/>
    <id>http://sanori.github.io/2019/08/Compare-Two-Tables-in-SQL/</id>
    <published>2019-08-05T17:50:59.000Z</published>
    <updated>2019-08-05T18:01:18.767Z</updated>
    
    <content type="html"><![CDATA[<p>You may want to compare the results processed while you develop and enhance data processing tasks.For example, you might want to see if the table from improved query are the same as the one of the existing query.If the tables are different, you might want to know how many rows are different and which field is different.In other words, you might want to compare two tables, such as the Linux <code>diff</code> command.</p><p>You might hesitate to compare two tables row by row if you feel the table is so big.But, don’t worry. You can make the cluster of servers compare the tables by simple SQL statement.</p><span id="more"></span><h2 id="Set-operators-in-SQL"><a href="#Set-operators-in-SQL" class="headerlink" title="Set operators in SQL"></a>Set operators in SQL</h2><p>You can use the SQL set operator to compare tables.For SQL set operators, the elements of a set are the rows.You may know the <code>UNION</code> operator which is frequently used to concatenate tables with the same schema. The SQL statement <code>A UNION B</code> returns the set union of the set <code>A</code> and <code>B</code>.Since the union operator is in SQL, it is natural that there are intersection and difference set operators in SQL.The former is <code>INTERSECT</code> and the latter is <code>EXCEPT</code>.The Venn diagram of SQL set operators are depicted as follows:</p><img src="/2019/08/Compare-Two-Tables-in-SQL/except-intersect-SQL.png" class="" title="EXCEPT, INTERSECT operator in SQL"><h2 id="EXCEPT-operator"><a href="#EXCEPT-operator" class="headerlink" title="EXCEPT operator"></a><code>EXCEPT</code> operator</h2><p>We can get the difference between the two tables using the <code>EXCEPT</code> operator.Note that <code>A EXCEPT B</code> returns only the rows that are in A but not in B.This statement does not return rows that are in A but not in B.Rows that are in A but not in B can be obtained with the statement ‘B EXCEPT A’.That is, you must combine the result of <code>EXCEPT B</code> with the result of<code> B EXCEPT A</code>.</p><p>As a result, you can get the difference between the two tables in the same way as the following SQL statement:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(</span><br><span class="line">    <span class="keyword">SELECT</span> &quot;A&quot; <span class="keyword">AS</span> TBL, <span class="operator">*</span></span><br><span class="line">    <span class="keyword">FROM</span> (</span><br><span class="line">        (<span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> A)</span><br><span class="line">        <span class="keyword">EXCEPT</span></span><br><span class="line">        (<span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> B)</span><br><span class="line">    )</span><br><span class="line">) <span class="keyword">UNION</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> &quot;B&quot; <span class="keyword">AS</span> TBL, <span class="operator">*</span></span><br><span class="line">    <span class="keyword">FROM</span> (</span><br><span class="line">        (<span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> B)</span><br><span class="line">        <span class="keyword">EXCEPT</span></span><br><span class="line">        (<span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> A)</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;You may want to compare the results processed while you develop and enhance data processing tasks.
For example, you might want to see if the table from improved query are the same as the one of the existing query.
If the tables are different, you might want to know how many rows are different and which field is different.
In other words, you might want to compare two tables, such as the Linux &lt;code&gt;diff&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;You might hesitate to compare two tables row by row if you feel the table is so big.
But, don’t worry. You can make the cluster of servers compare the tables by simple SQL statement.&lt;/p&gt;</summary>
    
    
    
    
    <category term="SQL" scheme="http://sanori.github.io/tags/SQL/"/>
    
    <category term="SparkSQL" scheme="http://sanori.github.io/tags/SparkSQL/"/>
    
  </entry>
  
  <entry>
    <title>JavaScript Pitfalls &amp; Tips: 2D Array, Matrix</title>
    <link href="http://sanori.github.io/2019/05/JavaScript-Pitfalls-Tips-2D-Array-Matrix/"/>
    <id>http://sanori.github.io/2019/05/JavaScript-Pitfalls-Tips-2D-Array-Matrix/</id>
    <published>2019-05-25T13:51:10.000Z</published>
    <updated>2019-08-05T18:01:18.767Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/05/JavaScript-Pitfalls-Tips-2D-Array-Matrix/js-2d-array-title.png" class="" title="const A &#x3D; Array(4).fill(Array(4).fill(0));"><p>Sometimes, you need to create and manipulate a two-dimensional (2D) array or a matrix. In JavaScript, an array of arrays can be used as a 2D array. </p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> mat = [</span><br><span class="line">  [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">  [<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">  [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]</span><br><span class="line">];</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(mat[<span class="number">1</span>][<span class="number">2</span>]);    <span class="comment">// gives 6</span></span><br></pre></td></tr></table></figure><p>How do you implement if you initialize m x n matrix with zeros?If you know about <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/fill"><code>Array.prototype.fill()</code></a>, you may be tempted to initialize m x n array like <code>Array(m).fill(Array(n).fill(0))</code>, but this does not work properly.Since the <code>Array(n).fill(0)</code> creates just <strong>one</strong> array of size <em>n</em> and all the elements of the first array, that represents rows, point to the same, the second array as follows:</p><p>When you got to know <code>Array.prototype.fill()</code> method, you may be tempted to initialize m x n array like <code>Array(m).fill(Array(n).fill(0))</code>, but this does not work properly. Since the <code>Array(n).fill(0)</code> creates just <strong>one</strong> array of size <em>n</em> and all the elements of the first array, that represents rows, point to the same, second array as follows:</p><span id="more"></span> <img src="/2019/05/JavaScript-Pitfalls-Tips-2D-Array-Matrix/js-2d-array-misleading.png" class="" title="the structure of Array(m).fill(Array(n).fill(0))"><p>For the same reason, “Array(m).fill([])” also does not work properly.</p><h2 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h2><h3 id="Initialize-each-row-using-for-loop"><a href="#Initialize-each-row-using-for-loop" class="headerlink" title="Initialize each row using for loop"></a>Initialize each row using for loop</h3><p>The simplest solution is to create each row using for loop:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> mat = <span class="title class_">Array</span>(m);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">let</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">  mat[i] = <span class="title class_">Array</span>(n).<span class="title function_">fill</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Using-map"><a href="#Using-map" class="headerlink" title="Using map"></a>Using <code>map</code></h3><p>If you are aware of functional programming, you would know that a for loop can be converted to map and&#x2F;or reduce. In this case, we can use a map function in this way.</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> mat = <span class="title class_">Array</span>(m).<span class="title function_">fill</span>(<span class="literal">null</span>).<span class="title function_">map</span>(<span class="function">() =&gt;</span> <span class="title class_">Array</span>(n).<span class="title function_">fill</span>(<span class="number">0</span>));</span><br></pre></td></tr></table></figure><h3 id="Using-Array-from"><a href="#Using-Array-from" class="headerlink" title="Using Array.from()"></a>Using <code>Array.from()</code></h3><p><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/from"><code>Array.from</code></a> can create using <a href="http://2ality.com/2013/05/quirk-array-like-objects.html">arrayLike object</a> and map function.It can simplify <code>Array(m).fill(null)</code> part of previous solution.</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> mat = <span class="title class_">Array</span>.<span class="title function_">from</span>(&#123;<span class="attr">length</span>:m&#125;, <span class="function">() =&gt;</span> <span class="title class_">Array</span>(n).<span class="title function_">fill</span>(<span class="number">0</span>));</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://stackoverflow.com/questions/966225/how-can-i-create-a-two-dimensional-array-in-javascript">How can I create a two dimensional array in JavaScript? - Stack Overflow</a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/fill">Array.prototype.fill() - JavaScript | MDN</a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/from">Array.from() - JavaScript | MDN</a></li><li><a href="http://2ality.com/2013/05/quirk-array-like-objects.html">JavaScript quirk 8: array-like objects</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2019/05/JavaScript-Pitfalls-Tips-2D-Array-Matrix/js-2d-array-title.png&quot; class=&quot;&quot; title=&quot;const A &amp;#x3D; Array(4).fill(Array(4).fill(0));&quot;&gt;

&lt;p&gt;Sometimes, you need to create and manipulate a two-dimensional (2D) array or a matrix. In JavaScript, an array of arrays can be used as a 2D array. &lt;/p&gt;
&lt;figure class=&quot;highlight js&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;const&lt;/span&gt; mat = [&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  [&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  [&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  [&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;11&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  [&lt;span class=&quot;number&quot;&gt;12&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;13&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;14&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;15&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;];&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;variable language_&quot;&gt;console&lt;/span&gt;.&lt;span class=&quot;title function_&quot;&gt;log&lt;/span&gt;(mat[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;]);    &lt;span class=&quot;comment&quot;&gt;// gives 6&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;How do you implement if you initialize m x n matrix with zeros?
If you know about &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/fill&quot;&gt;&lt;code&gt;Array.prototype.fill()&lt;/code&gt;&lt;/a&gt;, you may be tempted to initialize m x n array like &lt;code&gt;Array(m).fill(Array(n).fill(0))&lt;/code&gt;, but this does not work properly.
Since the &lt;code&gt;Array(n).fill(0)&lt;/code&gt; creates just &lt;strong&gt;one&lt;/strong&gt; array of size &lt;em&gt;n&lt;/em&gt; and all the elements of the first array, that represents rows, point to the same, the second array as follows:&lt;/p&gt;
&lt;p&gt;When you got to know &lt;code&gt;Array.prototype.fill()&lt;/code&gt; method, you may be tempted to initialize m x n array like &lt;code&gt;Array(m).fill(Array(n).fill(0))&lt;/code&gt;, but this does not work properly. Since the &lt;code&gt;Array(n).fill(0)&lt;/code&gt; creates just &lt;strong&gt;one&lt;/strong&gt; array of size &lt;em&gt;n&lt;/em&gt; and all the elements of the first array, that represents rows, point to the same, second array as follows:&lt;/p&gt;</summary>
    
    
    
    
    <category term="node.js" scheme="http://sanori.github.io/tags/node-js/"/>
    
    <category term="JavaScript" scheme="http://sanori.github.io/tags/JavaScript/"/>
    
    <category term="Matrix" scheme="http://sanori.github.io/tags/Matrix/"/>
    
    <category term="2DArray" scheme="http://sanori.github.io/tags/2DArray/"/>
    
  </entry>
  
  <entry>
    <title>JavaScript Pitfalls &amp; Tips: toFixed</title>
    <link href="http://sanori.github.io/2019/04/JavaScript-Pitfalls-Tips-toFixed/"/>
    <id>http://sanori.github.io/2019/04/JavaScript-Pitfalls-Tips-toFixed/</id>
    <published>2019-04-27T15:19:00.000Z</published>
    <updated>2019-04-27T16:57:51.172Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/04/JavaScript-Pitfalls-Tips-toFixed/js-toFixed-title.png" class="" title="2.55.toFixed(1) !&#x3D;&#x3D; 2.6"><p><code>Number.prototype.toFixed(n)</code> is a number formatting method that shows n-th digits after the decimal point. It seems to rounds (n+1)-th digits. But, sometimes, it does not round up. For example, the value of <code>2.55.toFixed(1)</code> is <code>2.5</code>, not <code>2.6</code> which is correct. <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/toFixed#Using_toFixed">This example is even in MDN documents</a>.</p><span id="more"></span><p>This strange phenomenon is due to the inaccuracy of floating points notation. What you get if you add 0.05 to 2.55? You must get a 2.6. But, in 64-bit floating points notation, the answer is as follows:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; 2.55 + 0.05</span><br><span class="line">2.5999999999999996</span><br></pre></td></tr></table></figure><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>If we want to round the digit correctly, we should not use <code>toFixed(n)</code>. There are the following alternatives.</p><h3 id="Do-it-in-Integer"><a href="#Do-it-in-Integer" class="headerlink" title="Do it in Integer"></a>Do it in Integer</h3><p>The first alternative is not using floating points, but using integer. For example, set all the currency unit to cents, not dollars.</p><p>Adding 0.5 and get the integer part in 64-bit floating points notation does no harm. Using this feature we can write another toFixed function as follows:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return the fixed point notation.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &#123;<span class="type">number</span>&#125; <span class="variable">n</span> - the number to be fixed notation </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &#123;<span class="type">number</span>&#125; <span class="variable">d</span> - the number of digits after decimal points</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@returns</span> &#123;<span class="type">string</span>&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">toFixed</span>(<span class="params">n, d</span>) &#123;</span><br><span class="line">  <span class="keyword">const</span> tenToD = <span class="title class_">Math</span>.<span class="title function_">pow</span>(<span class="number">10</span>, d);</span><br><span class="line">  <span class="keyword">return</span> (<span class="title class_">Math</span>.<span class="title function_">floor</span>(n * tenToD + <span class="number">0.5</span>) / tenToD).<span class="title function_">toFixed</span>(d);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>You may use <code>Math.round</code> instead of <code>Math.floor</code> and <code>+ 0.5</code>.</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">toFixed</span>(<span class="params">n, d</span>) &#123;</span><br><span class="line">  <span class="keyword">const</span> tenToD = <span class="title class_">Math</span>.<span class="title function_">pow</span>(<span class="number">10</span>, d);</span><br><span class="line">  <span class="keyword">return</span> (<span class="title class_">Math</span>.<span class="title function_">round</span>(n * tenToD) / tenToD).<span class="title function_">toFixed</span>(d);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Arbitrary-precision-libraries"><a href="#Arbitrary-precision-libraries" class="headerlink" title="Arbitrary precision libraries"></a>Arbitrary precision libraries</h3><p>If you need to calculate more precisely than 64-bit floating points notation, you would better use one of the arbitrary precision libraries. Most arbitrary precision libraries can designate precision in decimal places. Moreover, most of the libraries support <code>toFixed</code>.</p><p>For example, you can see the <code>toFixed</code> of <a href="http://mikemcl.github.io/big.js/">big.js</a> works correctly as follows:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="title class_">Big</span> = <span class="built_in">require</span>(<span class="string">&#x27;big.js&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> test = <span class="title class_">Big</span>(<span class="string">&#x27;2.55&#x27;</span>).<span class="title function_">toFixed</span>(<span class="number">1</span>);</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(test);            <span class="comment">// 2.6</span></span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(test === <span class="string">&#x27;2.6&#x27;</span>);  <span class="comment">// true</span></span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/toFixed">Number.prototype.toFixed() - JavaScript | MDN</a></li><li><a href="https://github.com/MikeMcl/big.js/wiki">What is the difference between big.js, bignumber.js and decimal.js?</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2019/04/JavaScript-Pitfalls-Tips-toFixed/js-toFixed-title.png&quot; class=&quot;&quot; title=&quot;2.55.toFixed(1) !&amp;#x3D;&amp;#x3D; 2.6&quot;&gt;

&lt;p&gt;&lt;code&gt;Number.prototype.toFixed(n)&lt;/code&gt; is a number formatting method that shows n-th digits after the decimal point. It seems to rounds (n+1)-th digits. But, sometimes, it does not round up. For example, the value of &lt;code&gt;2.55.toFixed(1)&lt;/code&gt; is &lt;code&gt;2.5&lt;/code&gt;, not &lt;code&gt;2.6&lt;/code&gt; which is correct. &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/toFixed#Using_toFixed&quot;&gt;This example is even in MDN documents&lt;/a&gt;.&lt;/p&gt;</summary>
    
    
    
    
    <category term="node.js" scheme="http://sanori.github.io/tags/node-js/"/>
    
    <category term="JavaScript" scheme="http://sanori.github.io/tags/JavaScript/"/>
    
    <category term="toFixed" scheme="http://sanori.github.io/tags/toFixed/"/>
    
  </entry>
  
  <entry>
    <title>Line-by-line Processing in node.js</title>
    <link href="http://sanori.github.io/2019/03/Line-by-line-Processing-in-node-js/"/>
    <id>http://sanori.github.io/2019/03/Line-by-line-Processing-in-node-js/</id>
    <published>2019-03-13T18:06:19.000Z</published>
    <updated>2019-03-13T18:25:08.463Z</updated>
    
    <content type="html"><![CDATA[<p>Suppose that you have to count the number of accesses for each IP address in the <code>access.log</code> file. And list the IP addresses which accessed more than 100,000 times.You want to do it with JavaScript or node.js. How can you do that?</p><span id="more"></span><p>You may write a script like as follows:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> fs = <span class="built_in">require</span>(<span class="string">&#x27;fs&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> lines = fs.<span class="title function_">readFileSync</span>(<span class="string">&#x27;access.log&#x27;</span>).<span class="title function_">toString</span>().<span class="title function_">split</span>(<span class="string">&#x27;\n&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> freq = &#123;&#125;;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">const</span> line <span class="keyword">of</span> lines) &#123;</span><br><span class="line">  <span class="keyword">const</span> ip = line.<span class="title function_">split</span>(<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>];</span><br><span class="line">  freq[ip] = (freq[ip] + <span class="number">1</span>) || <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> list = [];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">const</span> ip <span class="keyword">in</span> freq) &#123;</span><br><span class="line">  <span class="keyword">if</span> (freq[ip] &gt;= <span class="number">100000</span>) &#123;</span><br><span class="line">    list.<span class="title function_">push</span>([ip, freq[ip]]);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">list.<span class="title function_">sort</span>(<span class="function">(<span class="params">a,b</span>) =&gt;</span> b[<span class="number">1</span>] - a[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(list);</span><br></pre></td></tr></table></figure><p>This code works great for a small file.But, it would not work for a large file.It takes so much memory as the size of the <code>access.log</code> file since <code>readFileSync</code> returns the <code>lines</code> variable after reading all the file contents.Therefore, if the file is too large to fit in memory, the script does not work with the following error.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Error: Cannot create a string longer than 0x3fffffe7 characters</span><br><span class="line">    at Buffer.toString (buffer.js:645:17)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>Using <code>readFile()</code>, which is the asynchronous version of <code>readFileSync()</code>, would be a solution to the memory problem.But, the callback function would be called more than once and the passed <code>data</code> through callback function is not guaranteed to be passed line by line.</p><p>You need the way to read a file line by line, if it is possible, asynchronously.In this article, some ways to process text line by line are presented.</p><h2 id="readline-Standard-node-js-Module"><a href="#readline-Standard-node-js-Module" class="headerlink" title="readline: Standard node.js Module"></a><code>readline</code>: Standard node.js Module</h2><p>The standard node.js way to process text line by line is using the <a href="https://nodejs.org/api/readline.html">readline</a> module.</p><p>It seems that the major purpose of <code>readline</code> module is to make interactive text environment easily.But, we can make use of the feature to split the input stream by one line at a time.The rewritten script is as follows:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> fs = <span class="built_in">require</span>(<span class="string">&#x27;fs&#x27;</span>);</span><br><span class="line"><span class="keyword">const</span> readline = <span class="built_in">require</span>(<span class="string">&#x27;readline&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> rl = readline.<span class="title function_">createInterface</span>(&#123;</span><br><span class="line">  <span class="attr">input</span>: fs.<span class="title function_">createReadStream</span>(<span class="string">&#x27;access.log&#x27;</span>),</span><br><span class="line">  <span class="attr">crlfDelay</span>: <span class="title class_">Infinity</span></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> freq = &#123;&#125;;</span><br><span class="line">rl.<span class="title function_">on</span>(<span class="string">&#x27;line&#x27;</span>, <span class="function">(<span class="params">line</span>) =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">const</span> ip = line.<span class="title function_">split</span>(<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>];</span><br><span class="line">  freq[ip] = (freq[ip] + <span class="number">1</span>) || <span class="number">1</span>;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">rl.<span class="title function_">on</span>(<span class="string">&#x27;close&#x27;</span>, <span class="function">() =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">const</span> list = <span class="title class_">Object</span>.<span class="title function_">entries</span>(freq)</span><br><span class="line">    .<span class="title function_">filter</span>(<span class="function"><span class="params">x</span> =&gt;</span> x[<span class="number">1</span>] &gt;= <span class="number">100000</span>)</span><br><span class="line">    .<span class="title function_">sort</span>(<span class="function">(<span class="params">a,b</span>) =&gt;</span> b[<span class="number">1</span>] - a[<span class="number">1</span>]);</span><br><span class="line">  <span class="variable language_">console</span>.<span class="title function_">log</span>(list);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>Note that the line processing part, which was in the for-loop, is in the ‘line’ event handler.And, since it is asynchronous, the post-processing part should be in the ‘close’ event handler.</p><h2 id="split-Transform-Stream"><a href="#split-Transform-Stream" class="headerlink" title="split Transform Stream"></a><code>split</code> Transform Stream</h2><p>You may notice the event name of the ‘readline’ module is different from the standard event name if you are familiar to node.js stream.</p><p>If you just want to supply a line at a time to stream handler, you may use ‘<a href="https://www.npmjs.com/package/split"><code>split</code></a>‘ module.</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> fs = <span class="built_in">require</span>(<span class="string">&#x27;fs&#x27;</span>);</span><br><span class="line"><span class="keyword">const</span> split = <span class="built_in">require</span>(<span class="string">&#x27;split&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> freq = &#123;&#125;;</span><br><span class="line">fs.<span class="title function_">createReadStream</span>(<span class="string">&#x27;access.log&#x27;</span>)</span><br><span class="line">  .<span class="title function_">pipe</span>(<span class="title function_">split</span>())</span><br><span class="line">  .<span class="title function_">on</span>(<span class="string">&#x27;data&#x27;</span>, <span class="function">(<span class="params">line</span>) =&gt;</span> &#123;</span><br><span class="line">    <span class="keyword">const</span> ip = line.<span class="title function_">split</span>(<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>];</span><br><span class="line">    freq[ip] = (freq[ip] + <span class="number">1</span>) || <span class="number">1</span>;</span><br><span class="line">  &#125;)</span><br><span class="line">  .<span class="title function_">on</span>(<span class="string">&#x27;end&#x27;</span>, <span class="function">() =&gt;</span> &#123;</span><br><span class="line">    <span class="keyword">const</span> list = [];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> ip <span class="keyword">of</span> <span class="title class_">Object</span>.<span class="title function_">keys</span>(freq)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (freq[ip] &gt;= <span class="number">100000</span>) &#123;</span><br><span class="line">            list.<span class="title function_">push</span>([ip, freq[ip]]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    list.<span class="title function_">sort</span>(<span class="function">(<span class="params">a,b</span>) =&gt;</span> b[<span class="number">1</span>] - a[<span class="number">1</span>]);</span><br><span class="line">    <span class="variable language_">console</span>.<span class="title function_">log</span>(list);</span><br><span class="line">  &#125;);</span><br></pre></td></tr></table></figure><h2 id="readline-for-async-await"><a href="#readline-for-async-await" class="headerlink" title="readline for async&#x2F;await"></a><code>readline</code> for async&#x2F;await</h2><p>For who loves async&#x2F;await or Generator function,<a href="https://nodejs.org/dist/latest-v11.x/docs/api/readline.html#readline_rl_symbol_asynciterator">asyncIterator interface of ‘readline’</a>was experimentally added to node.js since v11.4.0.Using this feature, we can rewrite the script as follows:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> fs = <span class="built_in">require</span>(<span class="string">&#x27;fs&#x27;</span>);</span><br><span class="line"><span class="keyword">const</span> readline = <span class="built_in">require</span>(<span class="string">&#x27;readline&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">function</span> <span class="title function_">main</span>(<span class="params"></span>) &#123;</span><br><span class="line">  <span class="keyword">const</span> rl = readline.<span class="title function_">createInterface</span>(&#123;</span><br><span class="line">    <span class="attr">input</span>: fs.<span class="title function_">createReadStream</span>(<span class="string">&#x27;access.log&#x27;</span>),</span><br><span class="line">    <span class="attr">crlfDelay</span>: <span class="title class_">Infinity</span></span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> freq = &#123;&#125;;</span><br><span class="line">  <span class="keyword">for</span> <span class="keyword">await</span> (<span class="keyword">const</span> line <span class="keyword">of</span> rl) &#123;</span><br><span class="line">    <span class="keyword">const</span> ip = line.<span class="title function_">split</span>(<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>];</span><br><span class="line">    freq[ip] = (freq[ip] + <span class="number">1</span>) || <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> list = <span class="title class_">Object</span>.<span class="title function_">entries</span>(freq)</span><br><span class="line">    .<span class="title function_">filter</span>(<span class="function"><span class="params">x</span> =&gt;</span> x[<span class="number">1</span>] &gt;= <span class="number">100000</span>)</span><br><span class="line">    .<span class="title function_">sort</span>(<span class="function">(<span class="params">a,b</span>) =&gt;</span> b[<span class="number">1</span>] - a[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">  <span class="variable language_">console</span>.<span class="title function_">log</span>(list);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">require</span>.<span class="property">main</span> === <span class="variable language_">module</span>) &#123;</span><br><span class="line">  <span class="title function_">main</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Suppose that you have to count the number of accesses for each IP address in the &lt;code&gt;access.log&lt;/code&gt; file. And list the IP addresses which accessed more than 100,000 times.
You want to do it with JavaScript or node.js. How can you do that?&lt;/p&gt;</summary>
    
    
    
    
    <category term="node.js" scheme="http://sanori.github.io/tags/node-js/"/>
    
    <category term="JavaScript" scheme="http://sanori.github.io/tags/JavaScript/"/>
    
    <category term="CommandLineInterface" scheme="http://sanori.github.io/tags/CommandLineInterface/"/>
    
  </entry>
  
  <entry>
    <title>SQL Window Functions: row_number, rank, dense_rank</title>
    <link href="http://sanori.github.io/2018/12/SQL-Window-Functions-row-number-rank-dense-rank/"/>
    <id>http://sanori.github.io/2018/12/SQL-Window-Functions-row-number-rank-dense-rank/</id>
    <published>2018-12-17T16:54:39.000Z</published>
    <updated>2018-12-17T17:09:15.965Z</updated>
    
    <content type="html"><![CDATA[<p>SQL Window functions are the functions related to the ordering of elementsin each given partition.In <a href="/2018/11/SQL-Window-Functions-Top-k-Elements-Example/" title="SQL Window Functions: Top-k Elements Example">the last post</a>,I have introduced an example of getting top-3 elementsfor each partition using the <code>rank()</code> function.</p><p><code>row_number()</code>, <code>rank()</code> and <code>dense_rank()</code> are SQL Window functionsthat number each element in ordering. They look similar.In fact, the results of the functions are the sameif all the values of the ordering column are unique.But, the results of each function become different if there are the tie values,that is, the same elements in order.</p><span id="more"></span><p>By definition, <code>row_number()</code> just numbers each row through the ordering.Therefore, the result of <code>row_number()</code> is differenteven if the value of the ordering column is the same. <code>rank()</code> and <code>dense_rank()</code> return the rank of the ordering.Therefore, the resulting number of <code>rank()</code> is the samewhen the value of the ordering column is the same.But the next rank number is different.Assume that there are two top elements and the third one.<code>rank()</code> returns 3 for the third element but <code>dense_rank()</code> returns 2since the previous rank was 1.</p><p>Let’s see the differences by the examples. Assume that the contents of <code>rankEx</code> are as follows:</p><table><thead><tr><th>part</th><th>value</th></tr></thead><tbody><tr><td>1</td><td>aaa</td></tr><tr><td>1</td><td>aa</td></tr><tr><td>1</td><td>aab</td></tr><tr><td>1</td><td>aaa</td></tr><tr><td>2</td><td>aaa</td></tr><tr><td>2</td><td>bbb</td></tr><tr><td>2</td><td>bbb</td></tr><tr><td>2</td><td>cc</td></tr><tr><td>2</td><td>cc</td></tr></tbody></table><p>The result of <code>row_number()</code> is as follows:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">  <span class="built_in">row_number</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> part <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">value</span>) <span class="keyword">AS</span> num</span><br><span class="line"><span class="keyword">FROM</span> rankEx</span><br></pre></td></tr></table></figure><table><thead><tr><th>part</th><th>value</th><th>num</th></tr></thead><tbody><tr><td>1</td><td>aa</td><td>1</td></tr><tr><td>1</td><td>aaa</td><td>2</td></tr><tr><td>1</td><td>aaa</td><td>3</td></tr><tr><td>1</td><td>aab</td><td>4</td></tr><tr><td>2</td><td>aaa</td><td>1</td></tr><tr><td>2</td><td>bbb</td><td>2</td></tr><tr><td>2</td><td>bbb</td><td>3</td></tr><tr><td>2</td><td>cc</td><td>4</td></tr><tr><td>2</td><td>cc</td><td>5</td></tr></tbody></table><p>The result of <code>rank()</code> is as follows:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">  <span class="built_in">rank</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> part <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">value</span>) <span class="keyword">AS</span> num</span><br><span class="line"><span class="keyword">FROM</span> rankEx</span><br></pre></td></tr></table></figure><table><thead><tr><th>part</th><th>value</th><th>num</th></tr></thead><tbody><tr><td>1</td><td>aa</td><td>1</td></tr><tr><td>1</td><td>aaa</td><td>2</td></tr><tr><td>1</td><td>aaa</td><td>2</td></tr><tr><td>1</td><td>aab</td><td>4</td></tr><tr><td>2</td><td>aaa</td><td>1</td></tr><tr><td>2</td><td>bbb</td><td>2</td></tr><tr><td>2</td><td>bbb</td><td>2</td></tr><tr><td>2</td><td>cc</td><td>4</td></tr><tr><td>2</td><td>cc</td><td>4</td></tr></tbody></table><p>The result of <code>dense_rank()</code> is as follows:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span>,</span><br><span class="line">  <span class="built_in">dense_rank</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> part <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">value</span>) <span class="keyword">AS</span> num</span><br><span class="line"><span class="keyword">FROM</span> rankEx</span><br></pre></td></tr></table></figure><table><thead><tr><th>part</th><th>value</th><th>num</th></tr></thead><tbody><tr><td>1</td><td>aa</td><td>1</td></tr><tr><td>1</td><td>aaa</td><td>2</td></tr><tr><td>1</td><td>aaa</td><td>2</td></tr><tr><td>1</td><td>aab</td><td>3</td></tr><tr><td>2</td><td>aaa</td><td>1</td></tr><tr><td>2</td><td>bbb</td><td>2</td></tr><tr><td>2</td><td>bbb</td><td>2</td></tr><tr><td>2</td><td>cc</td><td>3</td></tr><tr><td>2</td><td>cc</td><td>3</td></tr></tbody></table><p><code>row_number()</code> is used instead of <code>rank()</code> or <code>dense_rank()</code> when the size of the results must be fixed. The statement <code>WHERE rank = 1</code> does not guarantee the number of results is 1 for each partition if the <code>rank</code> column is generated from <code>rank()</code> function.</p><p><code>row_number()</code> can be used to generate the sequence number of each row.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;SQL Window functions are the functions related to the ordering of elements
in each given partition.
In &lt;a href=&quot;/2018/11/SQL-Window-Functions-Top-k-Elements-Example/&quot; title=&quot;SQL Window Functions: Top-k Elements Example&quot;&gt;the last post&lt;/a&gt;,
I have introduced an example of getting top-3 elements
for each partition using the &lt;code&gt;rank()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;row_number()&lt;/code&gt;, &lt;code&gt;rank()&lt;/code&gt; and &lt;code&gt;dense_rank()&lt;/code&gt; are SQL Window functions
that number each element in ordering. They look similar.
In fact, the results of the functions are the same
if all the values of the ordering column are unique.
But, the results of each function become different if there are the tie values,
that is, the same elements in order.&lt;/p&gt;</summary>
    
    
    
    
    <category term="SQL" scheme="http://sanori.github.io/tags/SQL/"/>
    
    <category term="SparkSQL" scheme="http://sanori.github.io/tags/SparkSQL/"/>
    
    <category term="SqlWindowFunction" scheme="http://sanori.github.io/tags/SqlWindowFunction/"/>
    
  </entry>
  
  <entry>
    <title>SQL Window Functions: Top-k Elements Example</title>
    <link href="http://sanori.github.io/2018/11/SQL-Window-Functions-Top-k-Elements-Example/"/>
    <id>http://sanori.github.io/2018/11/SQL-Window-Functions-Top-k-Elements-Example/</id>
    <published>2018-11-03T13:52:21.000Z</published>
    <updated>2019-03-13T18:08:32.274Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/SQL_window_function">SQL Window functions</a>are similar to aggregate functions, such as count(),sum() or average(), but has different usage.Window functions are related to ordering like rank(), row_number(),while aggregate functions are related to summary of set of valueslike count(), sum().SQL Window functions specification is<a href="https://en.wikipedia.org/wiki/SQL:2003">ISO&#x2F;ANSI standard since 2003</a>.You can use SQL window functions on HiveQL, MySQL, Transact-SQLas well as Spark SQL.</p><p>In processing data, there are several cases to split data into groupsor partitions and get representative values for each group.Someone may solve this problem by classifying data by making keys firstand compute values for each key.You may use map() and reduce() in MapReduce framework,or use map() and reduceByKey() or aggregate() in Spark RDD API.</p><p>If the argument function of a reduce() is the add (<code>_ + _</code>),the query can be written as <code>sum()</code> in SQL.If it is the increment(<code>++</code>), it can be transformed to <code>count()</code> in SQL.But, if the function of the reduce() is a something like ‘top-10 largest values’,it would be vague to find correspondent SQL statements or functions.And some data engineers or scientists may think this query cannot be writtenin SQL statements, and decide to code in MapReduce or Spark.</p><p>In this post, I will show an example to solve top-<em>k</em> frequent elements problemusing the rank() function which is one of SQL window functions.</p><span id="more"></span><h2 id="Top-k-Frequent-Elements"><a href="#Top-k-Frequent-Elements" class="headerlink" title="Top-k Frequent Elements"></a>Top-<em>k</em> Frequent Elements</h2><p>Top-<em>k</em> frequent elements problem isfinding the most frequently appearing elements from the givensequence or array of elements,not only the most frequent one, but also <em>k</em>-th frequent element.</p><p>Assume that you are given web server log, <code>accessLog</code> in a table format like as follows:</p><table><thead><tr><th>remoteAddr</th><th>time</th><th>request</th><th align="right">status</th><th align="right">bytes</th></tr></thead><tbody><tr><td>172.19.0.1</td><td>2018-10-22T17:34:46Z</td><td>GET &#x2F; HTTP&#x2F;1.1</td><td align="right">200</td><td align="right">820</td></tr><tr><td>…</td><td>…</td><td>…</td><td align="right">…</td><td align="right">…</td></tr></tbody></table><p>If your work is find the most frequently visited remoteAddr and its hits,that is count of logs, how do you make a SQL query for the log?</p><h3 id="Top-k-most-frequent-remote-addresses"><a href="#Top-k-most-frequent-remote-addresses" class="headerlink" title="Top-k most frequent remote addresses"></a>Top-<em>k</em> most frequent remote addresses</h3><p>If the work requires the most 3-frequent remoteAddr’s for the whole log,the following query may suffice:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> remoteAddr, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">AS</span> hits</span><br><span class="line"><span class="keyword">FROM</span> accessLog</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> remoteAddr</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> hits <span class="keyword">DESC</span></span><br><span class="line">LIMIT <span class="number">3</span></span><br></pre></td></tr></table></figure><h3 id="Top-k-most-frequent-IPs-for-each-days"><a href="#Top-k-most-frequent-IPs-for-each-days" class="headerlink" title="Top-k most frequent IPs for each days"></a>Top-<em>k</em> most frequent IPs for each days</h3><p>But, if you need top-3 remoteAddr’s for each day,How do you query the log database?Here is the place where a window function make it.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="type">date</span>, remoteAddr, hits</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">  <span class="keyword">SELECT</span> <span class="type">date</span>, remoteAddr, hits,</span><br><span class="line">    <span class="built_in">rank</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="type">date</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> hits <span class="keyword">DESC</span>) <span class="keyword">AS</span> rank</span><br><span class="line">  <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> <span class="built_in">cast</span>(<span class="type">time</span> <span class="keyword">AS</span> <span class="type">date</span>) <span class="keyword">AS</span> <span class="type">date</span>, remoteAddr, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">AS</span> hits</span><br><span class="line">    <span class="keyword">FROM</span> accessLog</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="type">date</span>, remoteAddr</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WHERE</span> rank <span class="operator">&lt;=</span> <span class="number">3</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="type">date</span>, hits <span class="keyword">DESC</span></span><br></pre></td></tr></table></figure><p>How this works? Let’s see step by step from the innermost SELECT statement.</p><p>In line 6 ~ 8, the innermost SELECT counts hits for each date and remoteAddr,and generates an intermediate table like as following:</p><table><thead><tr><th>date</th><th>remoteAddr</th><th>hits</th></tr></thead><tbody><tr><td>2018-10-22</td><td>172.19.0.1</td><td>34867</td></tr><tr><td>2018-10-22</td><td>192.168.0.20</td><td>2334</td></tr><tr><td>2018-10-22</td><td>192.168.0.21</td><td>13223</td></tr><tr><td>2018-10-22</td><td>192.168.0.23</td><td>9385</td></tr><tr><td>2018-10-23</td><td>172.19.0.1</td><td>28762</td></tr><tr><td>2018-10-23</td><td>192.168.0.20</td><td>13534</td></tr><tr><td>2018-10-23</td><td>192.168.0.21</td><td>2331</td></tr><tr><td>2018-10-23</td><td>192.168.0.22</td><td>22937</td></tr></tbody></table><p>In line 3 ~ 4, rank() generates ranks by ‘hits’ for each ‘date’from the above table,and the rank value is added as an column named ‘rank’.</p><table><thead><tr><th>date</th><th>remoteAddr</th><th>hits</th><th>rank</th></tr></thead><tbody><tr><td>2018-10-22</td><td>172.19.0.1</td><td>34867</td><td>1</td></tr><tr><td>2018-10-22</td><td>192.168.0.20</td><td>2334</td><td>4</td></tr><tr><td>2018-10-22</td><td>192.168.0.21</td><td>13223</td><td>2</td></tr><tr><td>2018-10-22</td><td>192.168.0.23</td><td>9385</td><td>3</td></tr><tr><td>2018-10-23</td><td>172.19.0.1</td><td>28762</td><td>1</td></tr><tr><td>2018-10-23</td><td>192.168.0.20</td><td>13534</td><td>3</td></tr><tr><td>2018-10-23</td><td>192.168.0.21</td><td>2331</td><td>4</td></tr><tr><td>2018-10-23</td><td>192.168.0.22</td><td>22937</td><td>2</td></tr></tbody></table><p>In line 11, the rows with rank over 3 are omitted.</p><table><thead><tr><th>date</th><th>remoteAddr</th><th>hits</th><th>rank</th></tr></thead><tbody><tr><td>2018-10-22</td><td>172.19.0.1</td><td>34867</td><td>1</td></tr><tr><td>2018-10-22</td><td>192.168.0.21</td><td>13223</td><td>2</td></tr><tr><td>2018-10-22</td><td>192.168.0.23</td><td>9385</td><td>3</td></tr><tr><td>2018-10-23</td><td>172.19.0.1</td><td>28762</td><td>1</td></tr><tr><td>2018-10-23</td><td>192.168.0.20</td><td>13534</td><td>3</td></tr><tr><td>2018-10-23</td><td>192.168.0.22</td><td>22937</td><td>2</td></tr></tbody></table><p>In line 1, rank columns are removed.</p><table><thead><tr><th>date</th><th>remoteAddr</th><th>hits</th></tr></thead><tbody><tr><td>2018-10-22</td><td>172.19.0.1</td><td>34867</td></tr><tr><td>2018-10-22</td><td>192.168.0.21</td><td>13223</td></tr><tr><td>2018-10-22</td><td>192.168.0.23</td><td>9385</td></tr><tr><td>2018-10-23</td><td>172.19.0.1</td><td>28762</td></tr><tr><td>2018-10-23</td><td>192.168.0.20</td><td>13534</td></tr><tr><td>2018-10-23</td><td>192.168.0.22</td><td>22937</td></tr></tbody></table><p>In line 12, the rows are ordered by date and hits.Then you can get only most 3 frequent remoteAddr’s for each day.</p><table><thead><tr><th>date</th><th>remoteAddr</th><th>hits</th></tr></thead><tbody><tr><td>2018-10-22</td><td>172.19.0.1</td><td>34867</td></tr><tr><td>2018-10-22</td><td>192.168.0.21</td><td>13223</td></tr><tr><td>2018-10-22</td><td>192.168.0.23</td><td>9385</td></tr><tr><td>2018-10-23</td><td>172.19.0.1</td><td>28762</td></tr><tr><td>2018-10-23</td><td>192.168.0.22</td><td>22937</td></tr><tr><td>2018-10-23</td><td>192.168.0.20</td><td>13534</td></tr></tbody></table><p>The window function rank() generates rank of each days (date).And WHERE clause in outermost SELECT statement removes needless output.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li>SQL window function: <a href="https://en.wikipedia.org/wiki/SQL_window_function">https://en.wikipedia.org/wiki/SQL_window_function</a></li><li>Yin Huai and Michael Armbrust, “<a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html">Introducing Window Functions in Spark SQL</a>“, <em>Databricks Engneering Blog</em>, July 15, 2015, <a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a></li><li>Databricks Documentation &gt; <a href="https://docs.databricks.com/spark/latest/spark-sql/index.html">SQL Guide</a> &gt; <a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html">Select</a> &gt; <a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html#window-functions">Window functions</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SQL_window_function&quot;&gt;SQL Window functions&lt;/a&gt;
are similar to aggregate functions, such as count(),
sum() or average(), but has different usage.
Window functions are related to ordering like rank(), row_number(),
while aggregate functions are related to summary of set of values
like count(), sum().
SQL Window functions specification is
&lt;a href=&quot;https://en.wikipedia.org/wiki/SQL:2003&quot;&gt;ISO&amp;#x2F;ANSI standard since 2003&lt;/a&gt;.
You can use SQL window functions on HiveQL, MySQL, Transact-SQL
as well as Spark SQL.&lt;/p&gt;
&lt;p&gt;In processing data, there are several cases to split data into groups
or partitions and get representative values for each group.
Someone may solve this problem by classifying data by making keys first
and compute values for each key.
You may use map() and reduce() in MapReduce framework,
or use map() and reduceByKey() or aggregate() in Spark RDD API.&lt;/p&gt;
&lt;p&gt;If the argument function of a reduce() is the add (&lt;code&gt;_ + _&lt;/code&gt;),
the query can be written as &lt;code&gt;sum()&lt;/code&gt; in SQL.
If it is the increment(&lt;code&gt;++&lt;/code&gt;), it can be transformed to &lt;code&gt;count()&lt;/code&gt; in SQL.
But, if the function of the reduce() is a something like ‘top-10 largest values’,
it would be vague to find correspondent SQL statements or functions.
And some data engineers or scientists may think this query cannot be written
in SQL statements, and decide to code in MapReduce or Spark.&lt;/p&gt;
&lt;p&gt;In this post, I will show an example to solve top-&lt;em&gt;k&lt;/em&gt; frequent elements problem
using the rank() function which is one of SQL window functions.&lt;/p&gt;</summary>
    
    
    
    
    <category term="SQL" scheme="http://sanori.github.io/tags/SQL/"/>
    
    <category term="SparkSQL" scheme="http://sanori.github.io/tags/SparkSQL/"/>
    
    <category term="Spark" scheme="http://sanori.github.io/tags/Spark/"/>
    
    <category term="SqlWindowFunction" scheme="http://sanori.github.io/tags/SqlWindowFunction/"/>
    
  </entry>
  
  <entry>
    <title>JavaScript Pitfalls &amp; Tips: Bitwise Operation</title>
    <link href="http://sanori.github.io/2018/06/JavaScript-Pitfalls-Tips-Bitwise-Operation/"/>
    <id>http://sanori.github.io/2018/06/JavaScript-Pitfalls-Tips-Bitwise-Operation/</id>
    <published>2018-06-03T08:19:51.000Z</published>
    <updated>2018-11-26T17:19:27.924Z</updated>
    
    <content type="html"><![CDATA[<img src="/2018/06/JavaScript-Pitfalls-Tips-Bitwise-Operation/js-bitwise-title.png" class="" title="1 &lt;&lt; 31 !&#x3D;&#x3D; Math.pow(2, 31)"><p>Bitwise left shift operator (<code>&lt;&lt;</code>) is used instead of power of 2 (2<sup>n</sup>) operator on integer variable. This idiom may lead to strange result in JavaScript.</p><span id="more"></span><p>Bitwise operators in JavaScript converts their operands to <em>signed <strong>32</strong>-bit</em> integer, not 64-bit integer (also known as <a href="https://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html"><code>long</code></a> in Java, <a href="https://en.wikipedia.org/wiki/C_data_types#Basic_types"><code>long long</code></a> in C language).Therefore, <code>1 &lt;&lt; n</code> is not the same as <code>Math.pow(2, n)</code> if <code>n &gt; 31</code>.</p><p>Not only the shift operator but also all the bitwise operators such as and(<code>&amp;</code>), or(<code>|</code>), not(<code>~</code>), XOR(<code>^</code>) limited to process only 32-bit operands.</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><h3 id="Mathematical-operations"><a href="#Mathematical-operations" class="headerlink" title="Mathematical operations"></a>Mathematical operations</h3><p>You would better to use <code>Math.pow()</code> or <code>**</code> (ECMAScript 2016) than <code>&lt;&lt;</code> if you concern calculation.</p><p>Note that the safe integer range in JavaScript is -(2<sup>53</sup> - 1) inclusive to (2<sup>53</sup> - 1) inclusive since the <code>Number</code> type uses IEEE-754 64-bit floating point number representation.</p><h3 id="64-bit-Long-long-js"><a href="#64-bit-Long-long-js" class="headerlink" title="64-bit Long (long.js)"></a>64-bit Long (long.js)</h3><p>You may use <a href="https://www.npmjs.com/package/long">long</a> npm package if you concerned about only 64-bit wide integer.</p><h3 id="Bitmap-array"><a href="#Bitmap-array" class="headerlink" title="Bitmap array"></a>Bitmap array</h3><p>If you want to handle bit array with size &gt; 64, you would better consider bitmap array, also known as bit vector.</p><p>There are several npm packages about bit array such as <a href="https://www.npmjs.com/package/bitwise">bitwise</a>, <a href="https://www.npmjs.com/package/bitset">bitset</a>, <a href="https://www.npmjs.com/package/fast-bitset">fast-bitset</a>, <a href="https://www.npmjs.com/package/bit-vector">bit-vector</a>. You may choose as you need.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Bitwise_Operators">Bitwise operators - JavaScript|MDN</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2018/06/JavaScript-Pitfalls-Tips-Bitwise-Operation/js-bitwise-title.png&quot; class=&quot;&quot; title=&quot;1 &amp;lt;&amp;lt; 31 !&amp;#x3D;&amp;#x3D; Math.pow(2, 31)&quot;&gt;

&lt;p&gt;Bitwise left shift operator (&lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt;) is used instead of power of 2 (2&lt;sup&gt;n&lt;/sup&gt;) operator on integer variable. This idiom may lead to strange result in JavaScript.&lt;/p&gt;</summary>
    
    
    
    
    <category term="node.js" scheme="http://sanori.github.io/tags/node-js/"/>
    
    <category term="JavaScript" scheme="http://sanori.github.io/tags/JavaScript/"/>
    
    <category term="bitwise" scheme="http://sanori.github.io/tags/bitwise/"/>
    
  </entry>
  
  <entry>
    <title>JavaScript Pitfalls &amp; Tips: Sort</title>
    <link href="http://sanori.github.io/2018/05/JavaScript-Pitfalls-Tips-Sort/"/>
    <id>http://sanori.github.io/2018/05/JavaScript-Pitfalls-Tips-Sort/</id>
    <published>2018-05-24T18:51:03.000Z</published>
    <updated>2019-03-13T18:28:02.920Z</updated>
    
    <content type="html"><![CDATA[<p>JavaScript Quiz:</p><img src="/2018/05/JavaScript-Pitfalls-Tips-Sort/title.png" class="" title="[10, 2, 456, 3548].sort()"><p>What would be the result of the above JavaScript code? You may expect <code>[2, 10, 456, 3548]</code> since the array seem to be an integer array and JavaScript would sort it in numeric order. But, the correct result is as follows:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">10</span>, <span class="number">2</span>, <span class="number">3548</span>, <span class="number">456</span>]</span><br></pre></td></tr></table></figure><span id="more"></span><p>It is because the default comparison function of <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/sort"><code>Array.prototype.sort()</code></a> is the <em>string comparison</em> after converting each element to string.</p><p>Why the default sort function of JavaScript array as string comparison? I guess that the reason is the array type in JavaScript is for general types. For example, the following is a valid array of JavaScript.</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a = [ <span class="string">&#x27;13&#x27;</span>, <span class="string">&#x27;Bob&#x27;</span>, <span class="number">20</span>, <span class="title class_">NaN</span>, <span class="title class_">Infinity</span> ];</span><br></pre></td></tr></table></figure><p>Since the default sort function is for string, <code>a.sort()</code> works consistently and with no error.</p><h2 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h2><p>But, if you want to sort the array as numbers, how can you make it? There are two ways.</p><h3 id="Comparison-function-for-Number"><a href="#Comparison-function-for-Number" class="headerlink" title="Comparison function for Number"></a>Comparison function for <code>Number</code></h3><p>You may supply a comparison function for numbers as follows:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">10</span>, <span class="number">2</span>, <span class="number">456</span>, <span class="number">3548</span>].<span class="title function_">sort</span>(<span class="keyword">function</span>(<span class="params">a,b</span>) &#123; <span class="keyword">return</span> a - b &#125;)</span><br></pre></td></tr></table></figure><p>If you use ES2015 or above, you may use arrow function to keep it simple:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">10</span>, <span class="number">2</span>, <span class="number">456</span>, <span class="number">3548</span>].<span class="title function_">sort</span>(<span class="function">(<span class="params">a, b</span>) =&gt;</span> a - b)</span><br></pre></td></tr></table></figure><h3 id="Typed-array-ES2015-above"><a href="#Typed-array-ES2015-above" class="headerlink" title="Typed array (ES2015 above)"></a>Typed array (ES2015 above)</h3><p>You may think the sort function should sort in numeric order if the array is numeric typed array. You are right if you make the array numeric typed. JavaScript supports typed array.</p><p>ES2015 introduced <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/TypedArray"><code>TypedArray</code></a> as a view of binary data buffer. But you may use it as an array of the same type. So you can make the general array as typed array and sort as follows:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title class_">Float64Array</span>.<span class="title function_">from</span>([<span class="number">10</span>, <span class="number">2</span>, <span class="number">456</span>, <span class="number">3548</span>]).<span class="title function_">sort</span>()</span><br></pre></td></tr></table></figure><p>Note that there is <strong>no</strong> <code>Int64Array</code> type since the widest integer in bitwise operation is 32-bit.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;JavaScript Quiz:&lt;/p&gt;
&lt;img src=&quot;/2018/05/JavaScript-Pitfalls-Tips-Sort/title.png&quot; class=&quot;&quot; title=&quot;[10, 2, 456, 3548].sort()&quot;&gt;

&lt;p&gt;What would be the result of the above JavaScript code? You may expect &lt;code&gt;[2, 10, 456, 3548]&lt;/code&gt; since the array seem to be an integer array and JavaScript would sort it in numeric order. But, the correct result is as follows:&lt;/p&gt;
&lt;figure class=&quot;highlight js&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3548&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;456&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="node.js" scheme="http://sanori.github.io/tags/node-js/"/>
    
    <category term="JavaScript" scheme="http://sanori.github.io/tags/JavaScript/"/>
    
  </entry>
  
  <entry>
    <title>Running Keras directly on TensorFlow</title>
    <link href="http://sanori.github.io/2018/03/Running-Keras-directly-on-Tensorflow/"/>
    <id>http://sanori.github.io/2018/03/Running-Keras-directly-on-Tensorflow/</id>
    <published>2018-03-15T18:09:20.000Z</published>
    <updated>2019-08-05T18:01:18.767Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://keras.io/">Keras</a> is a high-level library&#x2F;API for neural network, a.k.a. deep learning. You can write shorter, simpler code using Keras. At the time of writing, Keras can use one of <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="http://deeplearning.net/software/theano/">Theano</a>, and <a href="https://www.microsoft.com/en-us/cognitive-toolkit/">CNTK</a> as a backend of deep learning process.</p><p>From TensorFlow 1.4, <a href="https://github.com/tensorflow/tensorflow/releases/tag/v1.4.0">Keras API became one of core APIs of TensorFlow</a>. Therefore, you don’t need to install both Keras and TensorFlow if you have a plan to use only TensorFlow backend in Keras. In other words, you can run Keras in simple way with full GPU support if you have got nvidia-docker environment which is mentioned in my last blog post, “<a href="/2017/02/Tensorflow-over-docker-with-GPU/">TensorFlow over docker with GPU support</a>“</p><p>In this post, I’ll show you how to modify original Keras code to run on TensorFlow directly.</p><span id="more"></span><h2 id="How-to-run-Keras-code-in-TensorFlow"><a href="#How-to-run-Keras-code-in-TensorFlow" class="headerlink" title="How to run Keras code in TensorFlow"></a>How to run Keras code in TensorFlow</h2><h3 id="Modify-import"><a href="#Modify-import" class="headerlink" title="Modify import"></a>Modify <code>import</code></h3><p><strong>Summary</strong>: Replace <code>keras</code> to <code>tensorflow.python.keras</code> at every <code>import</code> directive.</p><p>Since the Keras module in TensorFlow is <code>tf.keras</code>, some of you may try to convert the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br></pre></td></tr></table></figure><p>to</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tf.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tf.keras.layers <span class="keyword">import</span> Dense</span><br></pre></td></tr></table></figure><p>But, <strong>this would not work</strong>. You may come across the following error:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: No module named tf.keras.models</span><br></pre></td></tr></table></figure><p>Instead, the following works:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> Dense</span><br></pre></td></tr></table></figure><p>The reason is that the module location of <code>tf.keras</code> is <code>tensorflow/python/keras/__init__.py</code> as documented at <a href="https://www.tensorflow.org/api_docs/python/tf/keras">https://www.tensorflow.org/api_docs/python/tf/keras</a>. As the directory hierarchy, you should import <code>tensorflow.python.keras</code>, not <code>tf.keras</code>.</p><p><code>tf.keras</code> may be used if you use Keras API with TensorFlow style such as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">model = tf.keras.models.Sequential()</span><br><span class="line">model.add(tf.keras.layers.Dense(<span class="number">8</span>, input_dim=<span class="number">11</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br></pre></td></tr></table></figure><h3 id="Migrate-Keras-API"><a href="#Migrate-Keras-API" class="headerlink" title="Migrate Keras API"></a>Migrate Keras API</h3><p>You may skip this section if you already have the program written in Keras 2 API.</p><p><strong>Summary</strong>: Change <code>nb_epoch</code> to <code>epochs</code> in <code>model.fit()</code> method. And consult <a href="https://keras.io/">new API reference</a> for APIs that cause <code>Keyword argument not understood</code> error, and fix the argument to new one.</p><p>Some of early Keras documents, tutorials and books are written in <a href="https://faroit.github.io/keras-docs/1.2.2/">Keras 1 API</a> which is currently deprecated. Since the TensorFlow implementation of Keras API only supports Keras API version 2, you should change old API to the new one if your code is written in Keras 1.</p><p>The summary of changes from Keras 1 to Keras 2 is mentioned in “<a href="https://blog.keras.io/introducing-keras-2.html">Introducing Keras 2</a>“. According to the article, while The APIs are significantly changed, Keras gives warning or error message to help migrate to Keras 2. It is true for original Keras, but false for TensorFlow implementation. For example, note the following code which is wrtten in Keras 1 API:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.add(Dense(<span class="number">12</span>, input_dim=<span class="number">11</span>, init=<span class="string">&#x27;uniform&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br></pre></td></tr></table></figure><p>The (original) Keras gives the following warning to help migrating to Keras 2:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=&quot;relu&quot;, kernel_initializer=&quot;uniform&quot;, input_dim=11)`</span><br></pre></td></tr></table></figure><p>But, the Keras API of TensorFlow 1.6.0 gives the following error message:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: (&#x27;Keyword argument not understood:&#x27;, &#x27;init&#x27;)</span><br></pre></td></tr></table></figure><p>The most critical argument is <code>nb_epoch</code> in <code>model.fit()</code> since TensorFlow <strong>does not give any error messages</strong>. You should change <code>nb_epoch</code> to <code>epochs</code>. Without this, TensorFlow just run 1 epoch only.</p><h2 id="Migration-Example"><a href="#Migration-Example" class="headerlink" title="Migration Example"></a>Migration Example</h2><h3 id="Sample-Keras-1-code"><a href="#Sample-Keras-1-code" class="headerlink" title="Sample Keras 1 code"></a>Sample Keras 1 code</h3><p><a href="https://www.udemy.com/a-gentle-introduction-to-deep-learning-using-keras/"><em>“A Gentle Introduction to Deep Learning using Keras”</em></a> is a great introductory lecture about Keras. It is one hour lecture and easy to understand with explanatory example code in Jupyter notebook. You can get the iPython notebook and sample data from <a href="https://www.udemy.com/a-gentle-introduction-to-deep-learning-using-keras/learn/v4/t/lecture/6847974">Section 1, Lecture 9</a>.</p><p>The Keras code in the Lecture is written in Keras 1. So, you need to modify some python code to run it on TensorFlow directly.</p><h3 id="Run-TensorFlow-usnig-docker-image"><a href="#Run-TensorFlow-usnig-docker-image" class="headerlink" title="Run TensorFlow usnig docker image"></a>Run TensorFlow usnig docker image</h3><p>If you are not interested in docker, you can skip this section.</p><p>You can run TensorFlow instantly <strong>without</strong> installing Python, pip, pandas, TensorFlow, CUDA, cuDNN if you are using <a href="https://www.docker.com/">docker</a>.</p><p>After download and unzip <code>KerasDownload.zip</code>, you can get Jupyter environment by running official TensorFlow docker image:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -it --<span class="built_in">rm</span> \</span><br><span class="line">  -v <span class="string">&quot;<span class="variable">$PWD</span>&quot;</span>:/notebooks -p 8888:8888 \</span><br><span class="line">  gcr.io/tensorflow/tensorflow:latest-py3</span><br></pre></td></tr></table></figure><p>If you want to use your nVidia graphics card and have installed <code>nvidia-docker</code>, you can also get full support by GPU by running docker:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-docker run -it --<span class="built_in">rm</span> \</span><br><span class="line">  -v <span class="string">&quot;<span class="variable">$PWD</span>&quot;</span>:/notebooks -p 8888:8888 \</span><br><span class="line">  gcr.io/tensorflow/tensorflow:latest-gpu-py3</span><br></pre></td></tr></table></figure><h3 id="Modify-import-1"><a href="#Modify-import-1" class="headerlink" title="Modify import"></a>Modify <code>import</code></h3><p>In “Step 1. Import our modules”, there is import directive to import Keras classes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">import</span> numpy</span><br></pre></td></tr></table></figure><p>You need to change this to:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">import</span> numpy</span><br></pre></td></tr></table></figure><h3 id="Modify-Dense-and-fit"><a href="#Modify-Dense-and-fit" class="headerlink" title="Modify Dense and fit"></a>Modify <code>Dense</code> and <code>fit</code></h3><p>You may get an error in “Step 4. Build the Model” due to the argument <code>init</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">12</span>, input_dim=<span class="number">11</span>, init=<span class="string">&#x27;uniform&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(<span class="number">8</span>, init=<span class="string">&#x27;uniform&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, init=<span class="string">&#x27;uniform&#x27;</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br></pre></td></tr></table></figure><p>From Keras 2, <code>init</code> argument1 of <code>Dense</code> class is changed to <code>kernel_initializer</code>. (More precisely, <code>init</code> and <code>weights</code> are arranged to <code>kernel_initializer</code> and <code>bias_initializer</code>.) Therefore, you need to change the above code to:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">12</span>, input_dim=<span class="number">11</span>, kernel_initializer=<span class="string">&#x27;uniform&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(<span class="number">8</span>, kernel_initializer=<span class="string">&#x27;uniform&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>, kernel_initializer=<span class="string">&#x27;uniform&#x27;</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br></pre></td></tr></table></figure><p>In “Step 5. Fit the Model”, you should fix the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X, Y, nb_epoch=<span class="number">200</span>, batch_size=<span class="number">30</span>)</span><br></pre></td></tr></table></figure><p>In Keras 2, this code works, but not correctly. This code runs just 1 epoch which is the default value of epochs. If you want to run 200 epochs, you should fix it as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X, Y, epochs=<span class="number">200</span>, batch_size=<span class="number">30</span>)</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>You don’t need to install Keras in addtion to TensorFlow if you decided to use TensorFlow only as a backend since TensorFlow provides Keras API with small modification. In this post, I have showed how to run Keras on TensorFlow without install Keras package. I have also showed to migrate Keras 1 to Keras 2 API.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://stackoverflow.com/questions/47262955/how-to-import-keras-from-tf-keras-in-tensorflow">stackoverflow: How to import keras from tf.keras in TensorFlow?</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt; is a high-level library&amp;#x2F;API for neural network, a.k.a. deep learning. You can write shorter, simpler code using Keras. At the time of writing, Keras can use one of &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt;, &lt;a href=&quot;http://deeplearning.net/software/theano/&quot;&gt;Theano&lt;/a&gt;, and &lt;a href=&quot;https://www.microsoft.com/en-us/cognitive-toolkit/&quot;&gt;CNTK&lt;/a&gt; as a backend of deep learning process.&lt;/p&gt;
&lt;p&gt;From TensorFlow 1.4, &lt;a href=&quot;https://github.com/tensorflow/tensorflow/releases/tag/v1.4.0&quot;&gt;Keras API became one of core APIs of TensorFlow&lt;/a&gt;. Therefore, you don’t need to install both Keras and TensorFlow if you have a plan to use only TensorFlow backend in Keras. In other words, you can run Keras in simple way with full GPU support if you have got nvidia-docker environment which is mentioned in my last blog post, “&lt;a href=&quot;/2017/02/Tensorflow-over-docker-with-GPU/&quot;&gt;TensorFlow over docker with GPU support&lt;/a&gt;“&lt;/p&gt;
&lt;p&gt;In this post, I’ll show you how to modify original Keras code to run on TensorFlow directly.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Keras" scheme="http://sanori.github.io/tags/Keras/"/>
    
    <category term="TensorFlow" scheme="http://sanori.github.io/tags/TensorFlow/"/>
    
    <category term="nvidia-docker" scheme="http://sanori.github.io/tags/nvidia-docker/"/>
    
    <category term="docker" scheme="http://sanori.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Writing Spark batches only in SQL</title>
    <link href="http://sanori.github.io/2017/10/Writing-Spark-batches-only-in-SQL/"/>
    <id>http://sanori.github.io/2017/10/Writing-Spark-batches-only-in-SQL/</id>
    <published>2017-10-07T14:57:03.000Z</published>
    <updated>2018-11-26T17:19:27.924Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://spark.apache.org/">Apache Spark<sup>TM</sup></a> is known as popular big data framework which is faster than Hadoop MapReduce, easy-to-use, and fault-tolerant. Most of the Spark tutorials require readers to understand Scala, Java, or Python as base programming language. But, in my opinion, SQL is enough to write a spark batch script.</p><p>In this article, I will show that <strong>you can write Spark batches only in SQL if your input data is ready as structured dataset</strong>. This means that you don’t need to learn Scala or Python, RDD, DataFrame if your job can be expressed in SQL. Moreover, the expression power of SparkSQL may be stronger than you think.</p><p>The SQL scripts as follows can be found at <a href="https://github.com/sanori/spark-sql-example">https://github.com/sanori/spark-sql-example</a>. You can test them yourself.</p><span id="more"></span><h2 id="Spark-batches"><a href="#Spark-batches" class="headerlink" title="Spark batches"></a>Spark batches</h2><p>Typical Spark batches are a program that</p><ol><li>read data from data sources,</li><li>transform and calculate the data, and</li><li>save the result.</li></ol><p>Most of the Spark tutorials require Scala or Python (or R) programming language to write a Spark batch. For example, you may write a Python script to calculate the lines of each plays of Shakespeare when you are provided the full text in parquet format as follows. (Some codes are included for illustration purpose.)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> sf</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start Spark</span></span><br><span class="line">spark = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">&quot;pyspark example&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">basedir = os.path.dirname(os.path.realpath(__file__))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read all of Shakespeare&#x27;s plays</span></span><br><span class="line">df = spark.read.parquet(os.path.join(basedir, <span class="string">&quot;data/shakespeare.gz.parquet&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the schema to the console</span></span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate number of lines of each work (play)</span></span><br><span class="line">result = df \</span><br><span class="line">    .groupBy(<span class="string">&quot;play_name&quot;</span>) \</span><br><span class="line">    .agg(sf.count(<span class="string">&quot;line_id&quot;</span>).alias(<span class="string">&quot;lines&quot;</span>)) \</span><br><span class="line">    .orderBy(<span class="string">&quot;lines&quot;</span>, ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print a part of the result to the console</span></span><br><span class="line">result.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the result as one file in JSON Lines format</span></span><br><span class="line">result \</span><br><span class="line">    .repartition(<span class="number">1</span>) \</span><br><span class="line">    .write \</span><br><span class="line">    .json(os.path.join(basedir, <span class="string">&quot;length_of_play&quot;</span>), mode=<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Stop Spark</span></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure><p>The data calculation part of the above code is to get the <code>result</code> DataFrame, which is from line 20 to 23. This part can be written using SQL as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calculate number of lines of each work (play)</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;shakespeare&quot;</span>)  <span class="comment"># register df as shakespeare table</span></span><br><span class="line">result = spark.sql(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">SELECT play_name, count(line_id) AS lines</span></span><br><span class="line"><span class="string">FROM shakespeare</span></span><br><span class="line"><span class="string">GROUP BY play_name</span></span><br><span class="line"><span class="string">ORDER BY lines DESC</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br></pre></td></tr></table></figure><p>As I learn more about Spark, I realized that most of the data processing code can be replaced by SQL. And I found that most of the Spark batch script can be rewritten in SQL. For example, the above batch script can be rewritten in SQL as follows:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Read all of Shakespeare&#x27;s plays</span></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">VIEW</span> shakespeare</span><br><span class="line">  <span class="keyword">USING</span> parquet</span><br><span class="line">  OPTIONS (path &quot;data/shakespeare.gz.parquet&quot;);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Print the table schema and additional informations to the console</span></span><br><span class="line"><span class="keyword">DESCRIBE</span> EXTENDED shakespeare;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Calculate number of lines of each work and print to the console</span></span><br><span class="line"><span class="keyword">SELECT</span> play_name, <span class="built_in">count</span>(line_id) <span class="keyword">AS</span> lines</span><br><span class="line">  <span class="keyword">FROM</span> shakespeare</span><br><span class="line">  <span class="keyword">GROUP</span> <span class="keyword">BY</span> play_name</span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> lines <span class="keyword">DESC</span></span><br><span class="line">  LIMIT <span class="number">20</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Save the result as one file in JSON Lines format</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> IF <span class="keyword">EXISTS</span> lengthOfPlay;   <span class="comment">-- to overwrite, remove existing table</span></span><br><span class="line"><span class="keyword">SET</span> spark.sql.shuffle.partitions<span class="operator">=</span><span class="number">1</span>;   <span class="comment">-- to make single output file</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> lengthOfPlay</span><br><span class="line">  <span class="keyword">USING</span> json</span><br><span class="line">  LOCATION &quot;length_of_play&quot;</span><br><span class="line">  <span class="keyword">AS</span> <span class="keyword">SELECT</span> play_name, <span class="built_in">count</span>(line_id) <span class="keyword">AS</span> lines</span><br><span class="line">    <span class="keyword">FROM</span> shakespeare</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> play_name</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> lines <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure><p>The above SQL script can be executed by <code>spark-sql</code> which is included in default Spark distribution. This may imply that Spark creators consider SQL as one of the main programming language. In fact, most of the SQL references are from the official Spark programming guide named <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL, DataFrames and Datasets Guide</a>. To see the SQL examples, you should click <strong>“Sql”</strong> tab as follows:</p><img src="/2017/10/Writing-Spark-batches-only-in-SQL/spark-doc-sql.png" class="" title="SQL tab on Spark official document"><h2 id="Writing-SQL-only-script"><a href="#Writing-SQL-only-script" class="headerlink" title="Writing SQL only script"></a>Writing SQL only script</h2><p>The most problematic part of writing a Spark script only in SQL would be file operations. Most of the Spark tutorials suggest to use Scala or Python Language to read a data file and&#x2F;or save the result of data processing. But, there are several ways to read or write a data file in SparkSQL as follows.</p><p>The file path in the following examples can be a HDFS URI path. That is, you can write SQL script for cluster or distributed environment.</p><h3 id="Read-a-data-file"><a href="#Read-a-data-file" class="headerlink" title="Read a data file"></a>Read a data file</h3><h4 id="Read-directly-from-SQL-FROM"><a href="#Read-directly-from-SQL-FROM" class="headerlink" title="Read directly from SQL FROM"></a>Read directly from SQL <code>FROM</code></h4><p>You can simply read the data as a table using SQL <code>FROM</code> statement.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> play_name</span><br><span class="line"><span class="keyword">FROM</span> parquet.`data<span class="operator">/</span>shakespeare.gz.parquet`</span><br></pre></td></tr></table></figure><p>Reference: <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#run-sql-on-files-directly">http://spark.apache.org/docs/latest/sql-programming-guide.html#run-sql-on-files-directly</a></p><h4 id="Register-a-file-as-VIEW-or-TABLE"><a href="#Register-a-file-as-VIEW-or-TABLE" class="headerlink" title="Register a file as VIEW or TABLE"></a>Register a file as <code>VIEW</code> or <code>TABLE</code></h4><p>You may use data definition language such as <code>CREATE VIEW</code> or <code>CREATE TABLE</code> if you want to load data as a table or a view. I recommend to use <code>CREATE TEMPORARY VIEW</code> if you don’t want to modify the original data since a <code>TABLE</code> can be modified by <code>INSERT INTO</code>, <code>TRUNCATE</code>, etc.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">VIEW</span> shakespeare</span><br><span class="line"><span class="keyword">USING</span> parquet</span><br><span class="line">OPTIONS (path &quot;data/shakespeare.gz.parquet&quot;)</span><br></pre></td></tr></table></figure><p>Reference:</p><ul><li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#loading-data-programmatically">http://spark.apache.org/docs/latest/sql-programming-guide.html#loading-data-programmatically</a></li><li><a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/create-view.html">https://docs.databricks.com/spark/latest/spark-sql/language-manual/create-view.html</a></li></ul><h3 id="Write-a-result-to-a-file"><a href="#Write-a-result-to-a-file" class="headerlink" title="Write a result to a file"></a>Write a result to a file</h3><p>In Spark tutorials with Scala or Python, the processed result can be saved to a file using <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@write:org.apache.spark.sql.DataFrameWriter[T]"><code>write</code></a> API of Dataset. This is intuitive for most programmers.</p><p>In SQL, all the data are abstracted as tables. Therefore, you should create a table to save or write the resultant data, instead of searching for write API for SQL.</p><h4 id="CREATE-TABLE-AS-SELECT"><a href="#CREATE-TABLE-AS-SELECT" class="headerlink" title="CREATE TABLE AS SELECT"></a><code>CREATE TABLE AS SELECT</code></h4><p>If you save a new dataset, you may use <code>CREATE TABLE AS SELECT</code> statement.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> lengthOfPlay</span><br><span class="line">  <span class="keyword">USING</span> json</span><br><span class="line">  LOCATION &quot;length_of_play&quot;</span><br><span class="line">  <span class="keyword">AS</span> <span class="keyword">SELECT</span> play_name, <span class="built_in">count</span>(line_id) <span class="keyword">AS</span> lines</span><br><span class="line">    <span class="keyword">FROM</span> shakespeare</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> play_name</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> lines <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure><h4 id="INSERT-SELECT"><a href="#INSERT-SELECT" class="headerlink" title="INSERT SELECT"></a><code>INSERT SELECT</code></h4><p>If you want to append new data to existing table or directory,you may use <code>INSERT INTO &lt;table&gt; SELECT</code> statement.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> lengthOfPlay</span><br><span class="line">  <span class="keyword">SELECT</span> play_name, <span class="built_in">count</span>(line_id) <span class="keyword">AS</span> lines</span><br><span class="line">  <span class="keyword">FROM</span> shakespeare</span><br><span class="line">  <span class="keyword">GROUP</span> <span class="keyword">BY</span> play_name</span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> lines <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure><p>The table where the data are added must be created before to use <code>INSERT</code>.A typical table definition example is as follows.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> lengthOfPlay (</span><br><span class="line">  play_name STRING,</span><br><span class="line">  lines <span class="type">INT</span></span><br><span class="line">  )</span><br><span class="line">  <span class="keyword">USING</span> json</span><br><span class="line">  LOCATION &quot;length_of_play&quot;;</span><br></pre></td></tr></table></figure><p>For more information, consult <a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/insert.html">https://docs.databricks.com/spark/latest/spark-sql/language-manual/insert.html</a></p><p>You may consider <code>INSERT OVERWRITE TABLE</code> statement to create a new dataset.But, in my humble opinion, <code>CREATE TABLE AS SELECT</code> is better to create new datasetsince the table should be created before <code>INSERT</code> statement,and there is a chance of schema inconsistency between <code>CREATE TABLE</code> and <code>INSERT SELECT</code>.</p><p>Note that there is no <code>UPDATE</code> statement in SparkSQLsince all the data in Spark are immutable, or not changeable.You may create, append, or delete data, but you cannot change existing data.</p><h2 id="Running-SQL-only-script"><a href="#Running-SQL-only-script" class="headerlink" title="Running SQL only script"></a>Running SQL only script</h2><h3 id="spark-sql"><a href="#spark-sql" class="headerlink" title="spark-sql"></a><code>spark-sql</code></h3><p>If you have installed Spark, you can execute the SQL script file as follows:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$SPARK_HOME</span>/bin/spark-sql -f &lt;script.sql&gt;</span><br></pre></td></tr></table></figure><p>If you have added the spark executables to your <code>PATH</code>, <code>spark-sql -f &lt;script.sql&gt;</code> is enough.</p><p>Like <code>spark-submit</code>, <code>spark-sql</code> supports <code>--master</code>, <code>--conf</code>, <code>--executor-memory MEM</code>, <code>--executor-cores NUM</code> options. Therefore you can tune Spark parameters for better performance.</p><p>For more information on options and its meaning, consult <a href="http://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit">http://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit</a></p><h3 id="Setting-parameters-by-SET"><a href="#Setting-parameters-by-SET" class="headerlink" title="Setting parameters by SET"></a>Setting parameters by <code>SET</code></h3><p>You can set Spark parameters using <code>SET</code> statement in SparkSQL instead of setting parameters in <code>spark-sql</code> options. For example, you can replace <code>--master</code> option in <code>spark-sql</code> as follows.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> spark.master<span class="operator">=</span>&quot;local[*]&quot;;</span><br></pre></td></tr></table></figure><p>For more configuration variables, consult <a href="http://spark.apache.org/docs/latest/configuration.html#available-properties">http://spark.apache.org/docs/latest/configuration.html#available-properties</a></p><p>Reference: <a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/set.html">https://docs.databricks.com/spark/latest/spark-sql/language-manual/set.html</a></p><h2 id="Difference-from-RDBMS"><a href="#Difference-from-RDBMS" class="headerlink" title="Difference from RDBMS"></a>Difference from RDBMS</h2><p>Although Spark supports SQL including data definition language, Spark is <strong>NOT</strong> a relational DBMS. Spark just have taken SQL as data processing language.</p><p>All the data in Spark are immutable. That is, you <strong>cannot update</strong> existing tables, rows, and columns. There is no <code>UPDATE</code> statement.</p><h2 id="Motivation-of-this-article"><a href="#Motivation-of-this-article" class="headerlink" title="Motivation of this article"></a>Motivation of this article</h2><ul><li>Increasing Boilerplates in Spark batch scripts<ul><li>Boilerplate codes are increasing as time goes by and it makes hard to manage.</li><li>Most of Python&#x2F;Scala batches are the same except SQL part which process data.</li><li>Shell scripts are also required to submit Spark jobs using <code>spark-submit</code> with parameters.</li></ul></li><li>SparkSQL is similar to Dataset API<ul><li>Writing Dataset processing code using Dataset API is very similar to write SQL script.</li><li>Most of the Spark batches can be expressed in SQL language only.</li><li>SparkSQL examples exist in Dataset API.</li></ul></li><li>Dataset is recommended than RDD since Spark 2<ul><li>Dataset, which is general type of DataFrame, is supported by query optimizer (Catalyst) and efficient code generator and serializer (Tungsten).</li><li>It is recommended to use Dataset instead of RDD due to the efficiency when the developer is not a Spark expert.</li></ul></li></ul><h2 id="Further-Works"><a href="#Further-Works" class="headerlink" title="Further Works"></a>Further Works</h2><p>The expresion power of SparkSQL may be more than we think.</p><ul><li><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window$">Window functions</a></li><li><a href="https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html">Higher order functions</a></li></ul><p>I will write a article about these topics when I get time.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li>Spark SQL, DataFrames and Datasets Guide: <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a></li><li>Spark SQL Reference: <a href="https://docs.databricks.com/spark/latest/spark-sql/index.html">https://docs.databricks.com/spark/latest/spark-sql/index.html</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;Apache Spark&lt;sup&gt;TM&lt;/sup&gt;&lt;/a&gt; is known as popular big data framework which is faster than Hadoop MapReduce, easy-to-use, and fault-tolerant. Most of the Spark tutorials require readers to understand Scala, Java, or Python as base programming language. But, in my opinion, SQL is enough to write a spark batch script.&lt;/p&gt;
&lt;p&gt;In this article, I will show that &lt;strong&gt;you can write Spark batches only in SQL if your input data is ready as structured dataset&lt;/strong&gt;. This means that you don’t need to learn Scala or Python, RDD, DataFrame if your job can be expressed in SQL. Moreover, the expression power of SparkSQL may be stronger than you think.&lt;/p&gt;
&lt;p&gt;The SQL scripts as follows can be found at &lt;a href=&quot;https://github.com/sanori/spark-sql-example&quot;&gt;https://github.com/sanori/spark-sql-example&lt;/a&gt;. You can test them yourself.&lt;/p&gt;</summary>
    
    
    
    
    <category term="SQL" scheme="http://sanori.github.io/tags/SQL/"/>
    
    <category term="SparkSQL" scheme="http://sanori.github.io/tags/SparkSQL/"/>
    
    <category term="Spark" scheme="http://sanori.github.io/tags/Spark/"/>
    
    <category term="DataFrame" scheme="http://sanori.github.io/tags/DataFrame/"/>
    
    <category term="Dataset" scheme="http://sanori.github.io/tags/Dataset/"/>
    
  </entry>
  
  <entry>
    <title>Running Spark on Scala Worksheet</title>
    <link href="http://sanori.github.io/2017/07/Running-Spark-on-Scala-Worksheet/"/>
    <id>http://sanori.github.io/2017/07/Running-Spark-on-Scala-Worksheet/</id>
    <published>2017-06-30T18:15:53.000Z</published>
    <updated>2018-11-26T17:19:27.924Z</updated>
    
    <content type="html"><![CDATA[<p>In developing with Spark, you would want to try some queries or instructions and get results immediately to check if your idea is working or not. Sometimes, just trying to run the code would be faster than look through the reference manual.</p><p><a href="https://github.com/scala-ide/scala-worksheet/wiki/Getting-Started">Scala Worksheet</a> is a REPL(<a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop">Read-Eval-Print Loop</a>) on text editor in an IDE(integrated development environment) such as <a href="http://scala-ide.org/">Scala IDE</a> (on Eclipse) and <a href="https://www.jetbrains.com/idea/">IntelliJ IDEA</a>. You can try commands, functions, object definitions, and methods on Scala Worksheet(<code>.sc</code>) file, and you will get the results when you save that file. Since Scala worksheet runs on IDE, you can get help such as code completion.</p><p>In this post, a template to run Spark on Scala Worksheet is presented. The template was made to be general as possible.</p><span id="more"></span><img src="/2017/07/Running-Spark-on-Scala-Worksheet/spark-on-scala-worksheet.png" class="" title="Running Spark on Scala worksheet. (Scala IDE case) You can get its result on the right side every time when you save the file."><h2 id="Worksheet-Template"><a href="#Worksheet-Template" class="headerlink" title="Worksheet Template"></a>Worksheet Template</h2><p>If you copy-and-paste the following code to your Scala worksheet,you can interact with local Spark system immediately.The following code is for Spark 2.x.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123; <span class="type">Level</span>, <span class="type">Logger</span> &#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">sparkWorksheet</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Set off the vebose log messages</span></span><br><span class="line">  <span class="type">Logger</span>.getLogger(<span class="string">&quot;org&quot;</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">  <span class="type">Logger</span>.getLogger(<span class="string">&quot;akka&quot;</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder</span><br><span class="line">    .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    .appName(<span class="string">&quot;scalaWorksheet&quot;</span>)</span><br><span class="line">    .config(<span class="string">&quot;spark.ui.showConsoleProgress&quot;</span>, <span class="literal">false</span>)</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Your Spark codes are here</span></span><br><span class="line">  <span class="comment">// spark: sparkSession object (like sqlContext in Spark 1.x)</span></span><br><span class="line">  <span class="comment">// sc: sparkContext object (Spark 1.x compatible)</span></span><br><span class="line"></span><br><span class="line">  spark.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In line 7 and 8, suppress Spark log messages to simplify start up messages and resultant data. This also avoids <em>“Output exceeds cutoff limit”</em> error by reducing log messages.</p><p>In line 13, suppress progress bar graph which is intuitive in terminal environment. Without this, the result part would be messed up like as follows:</p><img src="/2017/07/Running-Spark-on-Scala-Worksheet/broken-worksheet.png" class="" title="Scala worksheet broken by progress bar"><p>You may increase the number of threads which run Spark by setting <code>.master(&quot;local[2]&quot;)</code> or <code>.master(&quot;local[*]&quot;)</code> in line 11. The details are in <a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls">Spark programming guides</a>.</p><h2 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h2><h3 id="“Output-exceeds-cutoff-limit”"><a href="#“Output-exceeds-cutoff-limit”" class="headerlink" title="“Output exceeds cutoff limit”"></a>“Output exceeds cutoff limit”</h3><p>This is caused by the limit of output buffer. You can expand the buffer size by changing IDE configuration.</p><ul><li>Scala IDE (Eclipse)In the <em>Windows</em> menu, select <em>Preference</em>, then you will get the Preference dialog. Select <em>Scala Worksheet</em> in Preference dialog, then you can change the <em>Output character limit per statement</em>.</li></ul><img src="/2017/07/Running-Spark-on-Scala-Worksheet/scala-worksheet-preferences-eclipse.png" class="" title="Scala worksheet preferences in Preferences dialog"><ul><li>IntelliJFile -&gt; Settings… -&gt; Languages and Frameworks -&gt; Scala -&gt; “Worksheet” tab -&gt; “Output cutoff limit, lines” option</li></ul><h3 id="Character-encoding-Spark-IDE-Eclipse"><a href="#Character-encoding-Spark-IDE-Eclipse" class="headerlink" title="Character encoding (Spark IDE, Eclipse)"></a>Character encoding (Spark IDE, Eclipse)</h3><p>The characters in some results may broken if your text are composed of not only ASCII characters, especially East Asian environment, such as Korean, Japanese, and Chinese. This problem may occurs in Microsoft Windows environment whose default character encoding is not UTF-8.</p><ul><li>Set JVM output character set as UTF-8Add the following line in <code>eclipse.ini</code>:<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Dfile.encoding=UTF8</span><br></pre></td></tr></table></figure></li><li>Set editor encoding as UTF-8In <em>Preferences</em> dialog, select General &gt; Workspace. If you scroll down the contents of Workspace preferences you can get <em>Text file encoding</em> area. Set the text file encoding as UTF-8.</li></ul><h2 id="Working-Examples"><a href="#Working-Examples" class="headerlink" title="Working Examples"></a>Working Examples</h2><p>(Update: 2016-07-03)</p><p>You can get the working examples of Scala worksheet that runs word count Spark code at <a href="https://github.com/sanori/spark-sbt">https://github.com/sanori/spark-sbt</a>.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://stackoverflow.com/questions/29896403/spark-output-log-style-vs-progress-style?answertab=votes#tab-top">stackoverflow: Spark ouput log style vs progress-style</a></li><li><a href="https://stackoverflow.com/questions/13409328/eclipse-output-exceeds-cutoff-limit-in-scala-worksheet">stackoverflow: Eclipse output exceeds cutoff limit in scala worksheet</a></li><li><a href="https://stackoverflow.com/questions/42269262/intellij-output-exceeds-cutoff-limit-in-scala-worksheet">stackoverflow: IntelliJ output exceeds cutoff limit in scala worksheet</a></li><li><a href="https://stackoverflow.com/questions/27781187/how-to-stop-messages-displaying-on-spark-console">stackoverflow: How to stop messages displaying on spark console?</a></li><li><a href="https://stackoverflow.com/questions/18470050/results-encoding-in-scala-worksheet-eclipse-plugin">stackoverflow: Results encoding in Scala Worksheet Eclipse plugin</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;In developing with Spark, you would want to try some queries or instructions and get results immediately to check if your idea is working or not. Sometimes, just trying to run the code would be faster than look through the reference manual.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/scala-ide/scala-worksheet/wiki/Getting-Started&quot;&gt;Scala Worksheet&lt;/a&gt; is a REPL(&lt;a href=&quot;https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop&quot;&gt;Read-Eval-Print Loop&lt;/a&gt;) on text editor in an IDE(integrated development environment) such as &lt;a href=&quot;http://scala-ide.org/&quot;&gt;Scala IDE&lt;/a&gt; (on Eclipse) and &lt;a href=&quot;https://www.jetbrains.com/idea/&quot;&gt;IntelliJ IDEA&lt;/a&gt;. You can try commands, functions, object definitions, and methods on Scala Worksheet(&lt;code&gt;.sc&lt;/code&gt;) file, and you will get the results when you save that file. Since Scala worksheet runs on IDE, you can get help such as code completion.&lt;/p&gt;
&lt;p&gt;In this post, a template to run Spark on Scala Worksheet is presented. The template was made to be general as possible.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Spark" scheme="http://sanori.github.io/tags/Spark/"/>
    
    <category term="Scala" scheme="http://sanori.github.io/tags/Scala/"/>
    
    <category term="Scala Worksheet" scheme="http://sanori.github.io/tags/Scala-Worksheet/"/>
    
    <category term="REPL" scheme="http://sanori.github.io/tags/REPL/"/>
    
    <category term="interactive development" scheme="http://sanori.github.io/tags/interactive-development/"/>
    
  </entry>
  
  <entry>
    <title>Using sbt instead of spark-shell</title>
    <link href="http://sanori.github.io/2017/06/Using-sbt-instead-of-spark-shell/"/>
    <id>http://sanori.github.io/2017/06/Using-sbt-instead-of-spark-shell/</id>
    <published>2017-06-06T09:17:11.000Z</published>
    <updated>2018-11-26T17:19:27.924Z</updated>
    
    <content type="html"><![CDATA[<p>If you are familiar to <a href="http://www.scala-sbt.org/0.13/docs/Command-Line-Reference.html#Configuration-level+tasks">sbt console</a>, a convenient Scala <a href="http://docs.scala-lang.org/overviews/repl/overview.html">REPL</a>, and you are about to develop <a href="http://spark.apache.org/">Spark</a> using <a href="http://spark.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell">spark-shell</a>, you don’t need to install spark-shell.</p><p>In fact, you don’t need to install even Spark!</p><span id="more"></span><h3 id="Include-Spark-in-build-sbt"><a href="#Include-Spark-in-build-sbt" class="headerlink" title="Include Spark in build.sbt"></a>Include Spark in <code>build.sbt</code></h3><p>Instead of download and install Spark, you can use spark by adding the following lines in your <code>build.sbt</code>.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">libraryDependencies += <span class="string">&quot;org.apache.spark&quot;</span> %% <span class="string">&quot;spark-core&quot;</span> % <span class="string">&quot;2.1.1&quot;</span>,</span><br><span class="line">libraryDependencies += <span class="string">&quot;org.apache.spark&quot;</span> %% <span class="string">&quot;spark-sql&quot;</span> % <span class="string">&quot;2.1.1&quot;</span>,</span><br></pre></td></tr></table></figure><p>Note that the last revision number, 2.1.1, is the version of Spark.You can check available spark modules at Maven repository.</p><ul><li><a href="https://mvnrepository.com/artifact/org.apache.spark">https://mvnrepository.com/artifact/org.apache.spark</a></li><li><a href="https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.11">https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.11</a></li></ul><h3 id="Generate-spark-and-sc-in-sbt-console"><a href="#Generate-spark-and-sc-in-sbt-console" class="headerlink" title="Generate spark and sc in sbt console"></a>Generate <code>spark</code> and <code>sc</code> in <code>sbt console</code></h3><p><code>spark-shell</code> provides <code>spark</code> (Spark 2.x) and <code>sc</code> object as the entry point of the Spark API. We can generate the API objects in sbt console as follows.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">&quot;local&quot;</span>).appName(<span class="string">&quot;spark-shell&quot;</span>).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br></pre></td></tr></table></figure><p>If you enter the above code in <code>sbt console</code>, you would get the Spark start messages. Note that the start message introduces the URI of SparkUI like as follows.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">17/06/07 04:18:48 INFO Utils: Successfully started service &#x27;SparkUI&#x27; on port 4040.</span><br><span class="line">17/06/07 04:18:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.2:4040</span><br></pre></td></tr></table></figure><h3 id="Automate-the-creation-of-Spark-API"><a href="#Automate-the-creation-of-Spark-API" class="headerlink" title="Automate the creation of Spark API"></a>Automate the creation of Spark API</h3><p>Instead of entering the above code every time when Scala REPL starts, you can automate the Spark API object creation by <a href="http://www.scala-sbt.org/0.13/docs/Howto-Scala.html#Define+the+initial+commands+evaluated+when+entering+the+Scala+REPL"><code>initialCommands</code> keyword in <code>build.sbt</code></a>.</p><p>By adding the following code in <code>build.sbt</code>, you can access <code>spark</code> and <code>sc</code> API object when <code>sbt console</code> starts.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">initialCommands in console := <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  import org.apache.spark.sql.SparkSession</span></span><br><span class="line"><span class="string">  import org.apache.spark.sql.functions._</span></span><br><span class="line"><span class="string">  val spark = SparkSession.builder()</span></span><br><span class="line"><span class="string">    .master(&quot;local&quot;)</span></span><br><span class="line"><span class="string">    .appName(&quot;spark-shell&quot;)</span></span><br><span class="line"><span class="string">    .getOrCreate()</span></span><br><span class="line"><span class="string">  import spark.implicits._</span></span><br><span class="line"><span class="string">  val sc = spark.sparkContext</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>Since the creation of <code>spark</code> object implies start of Spark system, it is needed to send shutdown signal to Spark when console is closed.</p><p>By adding the following code in <code>build.sbt</code>,Spark get exit signal of <code>sbt console</code> and shutdown gracefully.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cleanupCommands in console := <span class="string">&quot;spark.stop()&quot;</span></span><br></pre></td></tr></table></figure><h2 id="Get-Things-Together-Spark-SBT-template"><a href="#Get-Things-Together-Spark-SBT-template" class="headerlink" title="Get Things Together: Spark SBT template"></a>Get Things Together: Spark SBT template</h2><p>(Update: 2016-06-13)</p><p>Get the above comments altogether, I have created a SBT project template form spark.</p><p><a href="https://github.com/sanori/spark-sbt">https://github.com/sanori/spark-sbt</a></p><p>Hope this helps your work.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;If you are familiar to &lt;a href=&quot;http://www.scala-sbt.org/0.13/docs/Command-Line-Reference.html#Configuration-level+tasks&quot;&gt;sbt console&lt;/a&gt;, a convenient Scala &lt;a href=&quot;http://docs.scala-lang.org/overviews/repl/overview.html&quot;&gt;REPL&lt;/a&gt;, and you are about to develop &lt;a href=&quot;http://spark.apache.org/&quot;&gt;Spark&lt;/a&gt; using &lt;a href=&quot;http://spark.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell&quot;&gt;spark-shell&lt;/a&gt;, you don’t need to install spark-shell.&lt;/p&gt;
&lt;p&gt;In fact, you don’t need to install even Spark!&lt;/p&gt;</summary>
    
    
    
    
    <category term="Spark" scheme="http://sanori.github.io/tags/Spark/"/>
    
    <category term="Scala" scheme="http://sanori.github.io/tags/Scala/"/>
    
    <category term="sbt" scheme="http://sanori.github.io/tags/sbt/"/>
    
    <category term="spark-shell" scheme="http://sanori.github.io/tags/spark-shell/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow over docker with GPU support</title>
    <link href="http://sanori.github.io/2017/02/Tensorflow-over-docker-with-GPU/"/>
    <id>http://sanori.github.io/2017/02/Tensorflow-over-docker-with-GPU/</id>
    <published>2017-02-26T11:14:17.000Z</published>
    <updated>2018-11-26T17:19:27.924Z</updated>
    
    <content type="html"><![CDATA[<p>I have succeeded to run TensorFlow on my desktop which runs Ubuntu 16.04 LTS with nVidia GeForce GTX 1050. I want to share the easy and fast way to install TensorFlow, so I write this down. This does <strong>NOT</strong> require to download and install CUDA toolkit nor cuDNN, but you can get GPU acceleration.</p><span id="more"></span><h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><ul><li>nVidia graphics card (CUDA Compute Capability &gt;&#x3D; 3.0)<ul><li>You can check at <a href="https://developer.nvidia.com/cuda-gpus">nVidia documentation</a>.</li></ul></li><li>nVidia driver which supports your graphics card<ul><li>Ubuntu: <a href="https://launchpad.net/~graphics-drivers/+archive/ubuntu/ppa">ppa:graphics-drivers&#x2F;ppa</a></li></ul></li><li><a href="https://docs.docker.com/engine/installation/">Docker engine</a></li></ul><h2 id="Quick-start"><a href="#Quick-start" class="headerlink" title="Quick start"></a>Quick start</h2><ol><li>install <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a>.</li><li>run TensorFlow using nvidia-docker.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-docker run -it --<span class="built_in">rm</span> -p 8888:8888 gcr.io/tensorflow/tensorflow:latest-gpu</span><br></pre></td></tr></table></figure></li><li>&lt;ctrl&gt;-click or copy &amp; paste the URI looks like <code>http://localhost:8888/?token=&lt;hex_string&gt;</code> that appears on your terminal. Then you can get Jupyter notebook on your browser.<img src="/2017/02/Tensorflow-over-docker-with-GPU/TensorFlow-jupyter.png" class="" title="Jupyter Notebook of TensorFlow"></li></ol><h2 id="Why-this-works"><a href="#Why-this-works" class="headerlink" title="Why this works"></a>Why this works</h2><p><a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a> provides CUDA toolkit and cuDNN and tensorflow docker images utilizes it. All the installation procedures are abstracted by Docker images.</p><h2 id="Troubleshooting-in-Ubuntu"><a href="#Troubleshooting-in-Ubuntu" class="headerlink" title="Troubleshooting in Ubuntu"></a>Troubleshooting in Ubuntu</h2><ol><li>Identify your nVidia graphics card <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lspci | grep -i nvidia</span><br></pre></td></tr></table></figure></li><li>Verify CUDA driver and its version <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> /proc/driver/nvidia/version</span><br><span class="line">NVRM version: NVIDIA UNIX x86_64 Kernel Module  375.39  Tue Jan 31 20:47:00 PST 2017</span><br><span class="line">GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)</span><br></pre></td></tr></table></figure></li><li>Verify CUDA toolkit in <code>nvidia-docker</code> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nvidia-docker run --<span class="built_in">rm</span> nvidia/cuda nvidia-smi</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;I have succeeded to run TensorFlow on my desktop which runs Ubuntu 16.04 LTS with nVidia GeForce GTX 1050. I want to share the easy and fast way to install TensorFlow, so I write this down. This does &lt;strong&gt;NOT&lt;/strong&gt; require to download and install CUDA toolkit nor cuDNN, but you can get GPU acceleration.&lt;/p&gt;</summary>
    
    
    
    
    <category term="TensorFlow" scheme="http://sanori.github.io/tags/TensorFlow/"/>
    
    <category term="nvidia-docker" scheme="http://sanori.github.io/tags/nvidia-docker/"/>
    
    <category term="docker" scheme="http://sanori.github.io/tags/docker/"/>
    
    <category term="nVidia" scheme="http://sanori.github.io/tags/nVidia/"/>
    
    <category term="CUDA" scheme="http://sanori.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>Writing Command Line Scripts in Node.js</title>
    <link href="http://sanori.github.io/2015/09/Writing-Command-Line-Scripts-in-Node-js/"/>
    <id>http://sanori.github.io/2015/09/Writing-Command-Line-Scripts-in-Node-js/</id>
    <published>2015-09-21T18:28:30.000Z</published>
    <updated>2019-03-13T18:28:02.920Z</updated>
    
    <content type="html"><![CDATA[<p>I was new to node.js 4 month ago and my main programming language was Python and C. I was motivated to try node.js by the <a href="http://benchmarksgame.alioth.debian.org/u64/performance.php?test=regexdna">regex-dna benchmark</a> that shows JavaScript V8 is about 7 times faster than Python 3, and even faster than C implementation.</p><p>Jack Franklin’s article, <a href="http://javascriptplayground.com/blog/2015/03/node-command-line-tool/">“Writing Command Line Tools with Node”</a> was very helpful for me. The article shows that <code>npm</code> and <code>package.json</code> play an important role in running environment independent of OS, the importance of first line, basic command line argument processing and the way to invoke other commands.</p><p>In this post, I would like to share more tips to write scripts in node.js, JavaScript.</p><span id="more"></span><h2 id="Arguments-with-options-and-values"><a href="#Arguments-with-options-and-values" class="headerlink" title="Arguments with options and values"></a>Arguments with options and values</h2><p>Although <code>process.argv</code> is enough to process command line arguments, sometimes command line argument parser, such as <code>getopt</code> in C, <a href="https://docs.python.org/2.7/library/argparse.html#module-argparse">argparse</a> in Python is needed for productivity and usability, especially for processing options.</p><p><a href="https://github.com/tj/commander.js">commander.js</a> is perfect for this case. Just writing a help text for options as follows, make the parser works.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> program = <span class="built_in">require</span>(<span class="string">&#x27;commander&#x27;</span>);</span><br><span class="line">program</span><br><span class="line">    .<span class="title function_">option</span>(<span class="string">&#x27;-b, --boolean&#x27;</span>, <span class="string">&#x27;Boolean option&#x27;</span>)</span><br><span class="line">    .<span class="title function_">option</span>(<span class="string">&#x27;-k, --key [value]&#x27;</span>, <span class="string">&#x27;Key=value option&#x27;</span>)</span><br><span class="line">    .<span class="title function_">parse</span>(process.<span class="property">argv</span>);</span><br><span class="line"></span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;Boolean:&#x27;</span>, program.<span class="property">boolean</span>);</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;Key =&#x27;</span>, program.<span class="property">key</span>);</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;Rest arguments:&#x27;</span>, program.<span class="property">args</span>);</span><br></pre></td></tr></table></figure><p>There are alternatives for command line argument parsing:</p><ul><li><a href="https://github.com/substack/minimist">minimist</a>: Small module that returns compact JavaScript object as the result of argument parsing.</li><li><a href="https://github.com/chriso/cli">cli</a>: A toolkit for command line apps that contains several utilities including argument parser.</li></ul><h2 id="Detecting-the-script-is-called-in-command-line"><a href="#Detecting-the-script-is-called-in-command-line" class="headerlink" title="Detecting the script is called in command line"></a>Detecting the script is called in command line</h2><p>A script can be used by another script. But, running another node.js script through <code>child_prcess.exec</code> is somewhat hilarious. The best soultion would be import the script through <code>require</code>. But, we want the script works as the command also.</p><p>In Python, the folling idiom was used for a file that is used for both command to run and module to import. The <code>main</code> function will not be executed if it is imported as module.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>For example, <a href="https://docs.python.org/2/library/simplehttpserver.html">SimpleHTTPServer</a> module can be imported, but it can be run as simple http server by running <code>python &lt;path_to&gt;/SimpleHTTPServer.py</code> or <code>python -m SimpleHTTPServer</code>.</p><p>Similar idiom can be applied to node.js as follows. This is <a href="https://nodejs.org/api/modules.html#modules_accessing_the_main_module">a part of node.js specification</a> since <a href="https://nodejs.org/docs/v0.4.8/api/modules.html#accessing_the_main_module">node.js 0.4.8</a>.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="built_in">require</span>.<span class="property">main</span> === <span class="variable language_">module</span>) &#123;</span><br><span class="line">    <span class="title function_">main</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Synchronous-I-O"><a href="#Synchronous-I-O" class="headerlink" title="Synchronous I&#x2F;O"></a>Synchronous I&#x2F;O</h2><p>The most frustrating feature of node.js would be asynchronous, non-blocking I&#x2F;O especially for script writers in shell, python, etc. Since node 0.12, there exists synchronous API such as <a href="https://nodejs.org/api/child_process.html#child_process_child_process_execsync_command_options"><code>child_prcess.execSync</code></a> and <a href="https://nodejs.org/api/fs.html"><code>fs.*Sync</code></a>. Therefore you may write the scripts as you did in shell or python. But, since the virtue of node.js is fast from asynchronous I&#x2F;O, I recommend to try asynchronous features of node.js if you have enough time.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;I was new to node.js 4 month ago and my main programming language was Python and C. I was motivated to try node.js by the &lt;a href=&quot;http://benchmarksgame.alioth.debian.org/u64/performance.php?test=regexdna&quot;&gt;regex-dna benchmark&lt;/a&gt; that shows JavaScript V8 is about 7 times faster than Python 3, and even faster than C implementation.&lt;/p&gt;
&lt;p&gt;Jack Franklin’s article, &lt;a href=&quot;http://javascriptplayground.com/blog/2015/03/node-command-line-tool/&quot;&gt;“Writing Command Line Tools with Node”&lt;/a&gt; was very helpful for me. The article shows that &lt;code&gt;npm&lt;/code&gt; and &lt;code&gt;package.json&lt;/code&gt; play an important role in running environment independent of OS, the importance of first line, basic command line argument processing and the way to invoke other commands.&lt;/p&gt;
&lt;p&gt;In this post, I would like to share more tips to write scripts in node.js, JavaScript.&lt;/p&gt;</summary>
    
    
    
    
    <category term="node.js" scheme="http://sanori.github.io/tags/node-js/"/>
    
    <category term="JavaScript" scheme="http://sanori.github.io/tags/JavaScript/"/>
    
    <category term="CommandLineInterface" scheme="http://sanori.github.io/tags/CommandLineInterface/"/>
    
    <category term="CLI" scheme="http://sanori.github.io/tags/CLI/"/>
    
  </entry>
  
</feed>
